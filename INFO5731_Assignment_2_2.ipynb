{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhignya-Jagathpally/Abhignya_INFO5731/blob/main/INFO5731_Assignment_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f76ec54c32245fd92c8717dd8295053",
            "a35880903fa045369aceb99f3beb0b3b",
            "f611d822b52742a5b345a39e5c4d8dfe",
            "2446027c5d06448a81a02c75ba0895eb",
            "c9c12565bed1484db0e2f0783732ce2d",
            "24f652612edc4d1d832e2412061bbe02",
            "a31b5603fd424ac7b6f82756818bd1c6",
            "55ebe9fb07254c0aa31ee43380966e26",
            "5e5a3561dee64080acf8658c16e17bcc",
            "964bdf5d08e54307bdcd9499ec57a755",
            "814292dfbf92466abddf8fd45f47958b"
          ]
        },
        "outputId": "51ef24ce-d34a-49f9-c11e-4a143590764c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting simple Densho data collection...\n",
            "Starting simple data collection...\n",
            "Total records to collect: 1009\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Collecting narrators:   0%|          | 0/1009 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f76ec54c32245fd92c8717dd8295053"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=0\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=25\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=50\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=75\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=100\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=125\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=150\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=175\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=200\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=225\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=250\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=275\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=300\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=325\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=350\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=375\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=400\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=425\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=450\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=475\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=500\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=525\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=550\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=575\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=600\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=625\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=650\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=675\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=700\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=725\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=750\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=775\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=800\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=825\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=850\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=875\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=900\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=925\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=950\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=975\n",
            "Fetching: https://ddr.densho.org/api/0.2/narrator/?limit=50&offset=1000\n",
            "Reached end of data\n",
            "\n",
            " Successfully collected 1009 narrator records!\n",
            " Saved to /content/sample_data/densho_narrators_raw.csv\n",
            "\n",
            " Data Summary:\n",
            "   Total records: 1009\n",
            "   Columns: 12\n",
            "   Memory usage: 1337.4 KB\n",
            "\n",
            " Sample Data:\n",
            "     id                   name generation             birth_location\n",
            "0   361           Kay Aiko Abe      Nisei        Selleck, Washington\n",
            "1   291                Art Abe      Nisei        Seattle, Washington\n",
            "2   293  Sharon Tanagi Aburano      Nisei        Seattle, Washington\n",
            "3   597        Toshiko Aiboshi      Nisei  Boyle Heights, California\n",
            "4  1014      Douglas L. Aihara     Sansei       Torrance, California\n",
            "\n",
            " Missing Values:\n",
            "   birth_date: 74 (7.3%)\n",
            "   death_date: 451 (44.7%)\n",
            "\n",
            " Data collection complete! File saved as '/content/sample_data/densho_narrators_raw.csv'\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip -q install pandas tqdm\n",
        "\n",
        "import urllib.request\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def simple_fetch(url):\n",
        "    \"\"\"\n",
        "    Simplest possible fetch - no fancy headers, no compression handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create the most basic request possible\n",
        "        request = urllib.request.Request(url)\n",
        "\n",
        "        # Just add a basic User-Agent\n",
        "        request.add_header('User-Agent', 'Mozilla/5.0 (compatible; DataCollector 1.0)')\n",
        "\n",
        "        with urllib.request.urlopen(request, timeout=30) as response:\n",
        "            # Read raw bytes\n",
        "            content = response.read()\n",
        "\n",
        "            # Try to decode as UTF-8, with fallback\n",
        "            try:\n",
        "                text = content.decode('utf-8')\n",
        "            except UnicodeDecodeError:\n",
        "                # If UTF-8 fails, try latin-1 (which accepts any byte)\n",
        "                text = content.decode('latin-1')\n",
        "\n",
        "            # Parse JSON\n",
        "            return json.loads(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(f\"URL: {url}\")\n",
        "        return None\n",
        "\n",
        "def collect_all_narrators():\n",
        "    \"\"\"\n",
        "    Collect all narrator data with minimal complexity\n",
        "    \"\"\"\n",
        "    base_url = \"https://ddr.densho.org/api/0.2/narrator/\"\n",
        "    all_narrators = []\n",
        "    offset = 0\n",
        "    limit = 50  # Start with smaller batches\n",
        "\n",
        "    print(\"Starting simple data collection...\")\n",
        "\n",
        "    # First, get total count\n",
        "    first_url = f\"{base_url}?limit=1&offset=0\"\n",
        "    first_data = simple_fetch(first_url)\n",
        "\n",
        "    if not first_data:\n",
        "        print(\" Could not fetch initial data\")\n",
        "        return []\n",
        "\n",
        "    total_records = first_data.get('total', 0)\n",
        "    print(f\"Total records to collect: {total_records}\")\n",
        "\n",
        "    # Setup progress bar\n",
        "    pbar = tqdm(total=total_records, desc=\"Collecting narrators\")\n",
        "\n",
        "    while True:\n",
        "        # Build URL with parameters\n",
        "        url = f\"{base_url}?limit={limit}&offset={offset}\"\n",
        "\n",
        "        print(f\"Fetching: {url}\")\n",
        "        data = simple_fetch(url)\n",
        "\n",
        "        if not data:\n",
        "            print(f\"Failed to fetch data at offset {offset}\")\n",
        "            break\n",
        "\n",
        "        # Get the narrator objects\n",
        "        narrators = data.get('objects', [])\n",
        "\n",
        "        if not narrators:\n",
        "            print(\"No more narrators found\")\n",
        "            break\n",
        "\n",
        "        # Process each narrator\n",
        "        for narrator in narrators:\n",
        "            links = narrator.get('links', {})\n",
        "\n",
        "            narrator_data = {\n",
        "                'id': narrator.get('id'),\n",
        "                'name': narrator.get('display_name'),\n",
        "                'bio_text': narrator.get('bio'),\n",
        "                'generation': narrator.get('generation'),\n",
        "                'birth_location': narrator.get('birth_location'),\n",
        "                'birth_date': narrator.get('b_date'),\n",
        "                'death_date': narrator.get('d_date'),\n",
        "                'page_url': links.get('html'),\n",
        "                'json_url': links.get('json'),\n",
        "                'interviews_api': links.get('interviews'),\n",
        "                'image_url': links.get('img'),\n",
        "                'thumb_url': links.get('thumb'),\n",
        "            }\n",
        "\n",
        "            all_narrators.append(narrator_data)\n",
        "\n",
        "        # Update progress\n",
        "        pbar.update(len(narrators))\n",
        "\n",
        "        # Check if we're done\n",
        "        next_offset = data.get('next_offset')\n",
        "        if next_offset is None or next_offset <= offset:\n",
        "            print(\"Reached end of data\")\n",
        "            break\n",
        "\n",
        "        # Be polite - wait between requests\n",
        "        time.sleep(1)\n",
        "        offset = next_offset\n",
        "\n",
        "    pbar.close()\n",
        "    return all_narrators\n",
        "\n",
        "# ===== RUN THE COLLECTION =====\n",
        "\n",
        "print(\" Starting simple Densho data collection...\")\n",
        "\n",
        "narrators = collect_all_narrators()\n",
        "\n",
        "if narrators:\n",
        "    print(f\"\\n Successfully collected {len(narrators)} narrator records!\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(narrators)\n",
        "\n",
        "    # Save to CSV\n",
        "    filename = \"/content/sample_data/densho_narrators_raw.csv\"\n",
        "    df.to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\" Saved to {filename}\")\n",
        "\n",
        "    # Show some stats\n",
        "    print(f\"\\n Data Summary:\")\n",
        "    print(f\"   Total records: {len(df)}\")\n",
        "    print(f\"   Columns: {len(df.columns)}\")\n",
        "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
        "\n",
        "    # Show sample data\n",
        "    print(f\"\\n Sample Data:\")\n",
        "    pd.set_option('display.max_colwidth', 50)\n",
        "    print(df[['id', 'name', 'generation', 'birth_location']].head())\n",
        "\n",
        "    # Check for missing values\n",
        "    print(f\"\\n Missing Values:\")\n",
        "    missing = df.isnull().sum()\n",
        "    for col, count in missing.items():\n",
        "        if count > 0:\n",
        "            pct = (count / len(df)) * 100\n",
        "            print(f\"   {col}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n Data collection complete! File saved as '{filename}'\")\n",
        "\n",
        "else:\n",
        "    print(\" No data was collected\")\n",
        "    print(\"\\nTroubleshooting suggestions:\")\n",
        "    print(\"1. Try running this code on your local machine\")\n",
        "    print(\"2. Check your internet connection\")\n",
        "    print(\"3. The API might be temporarily unavailable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Your code here\n",
        "!pip -q install pandas nltk unidecode\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# NLTK assets (first run downloads)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Add this line to fix the error\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Load the CSV from Cell 1\n",
        "in_csv = \"/content/sample_data/densho_narrators_raw.csv\"\n",
        "df = pd.read_csv(in_csv)\n",
        "\n",
        "# We'll clean 'bio_text' (fall back to name if bio is empty)\n",
        "def pick_source_text(row):\n",
        "    txt = str(row.get(\"bio_text\") or \"\").strip()\n",
        "    return txt if txt else str(row.get(\"name\") or \"\")\n",
        "\n",
        "df[\"source_text\"] = df.apply(pick_source_text, axis=1)\n",
        "\n",
        "# Prepare resources\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_basic(text: str) -> str:\n",
        "    \"\"\"\n",
        "    (4) Lowercase\n",
        "    + ASCII fold (normalize accents)\n",
        "    + (1)(2) Remove punctuation/special chars AND numbers (keep spaces)\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = unidecode(text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)   # drop digits and punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "\n",
        "def clean_pipeline(text: str):\n",
        "    # Normalize\n",
        "    norm = normalize_basic(text)\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(norm) if norm else []\n",
        "    # (3) Remove stopwords\n",
        "    nostop = remove_stop_words(tokens)\n",
        "    # (5) Stemming\n",
        "    stemmed = [stemmer.stem(t) for t in nostop]\n",
        "    # (6) Lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in nostop]\n",
        "    return {\n",
        "        \"clean_norm\": norm,\n",
        "        \"clean_nostop\": \" \".join(nostop),\n",
        "        \"stemmed_text\": \" \".join(stemmed),\n",
        "        \"lemmatized_text\": \" \".join(lemmatized),\n",
        "        # Final preferred cleaned text:\n",
        "        \"clean_text\": \" \".join(lemmatized),\n",
        "    }\n",
        "\n",
        "# Apply the cleaning pipeline\n",
        "clean = df[\"source_text\"].fillna(\"\").map(clean_pipeline)\n",
        "clean_df = pd.DataFrame(list(clean))\n",
        "\n",
        "# Merge and save\n",
        "out_df = pd.concat([df, clean_df], axis=1)\n",
        "out_csv = \"densho_narrators_clean.csv\"\n",
        "out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"Saved cleaned data with new columns to {out_csv}\")\n",
        "\n",
        "# Show a preview (code + visible output)\n",
        "cols = [\"id\",\"name\",\"source_text\",\"clean_text\",\"stemmed_text\",\"lemmatized_text\"]\n",
        "pd.set_option(\"display.max_colwidth\", 100)\n",
        "display(out_df.head(5)[cols])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "EahJ4FB7I2zl",
        "outputId": "f5597006-6e0f-4c3b-e787-5d99c9e2436c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSaved cleaned data with new columns to densho_narrators_clean.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     id                   name  \\\n",
              "0   361           Kay Aiko Abe   \n",
              "1   291                Art Abe   \n",
              "2   293  Sharon Tanagi Aburano   \n",
              "3   597        Toshiko Aiboshi   \n",
              "4  1014      Douglas L. Aihara   \n",
              "\n",
              "                                                                                           source_text  \\\n",
              "0  Nisei female. Born May 9, 1927, in Selleck, Washington. Spent much of childhood in Beaverton, Or...   \n",
              "1  Nisei male. Born June 12, 1921, in Seattle, Washington. Grew up in an area of Seattle with few o...   \n",
              "2  Nisei female. Born October 31, 1925, in Seattle, Washington. Family owned and operated a success...   \n",
              "3  Nisei female. Born July 8, 1928, in Boyle Heights, California. At an early age, went to live wit...   \n",
              "4  Sansei male. Born March 15, 1950, in Torrance, California. Grew up in the Los Angeles area, wher...   \n",
              "\n",
              "                                                                                            clean_text  \\\n",
              "0  nisei female born may selleck washington spent much childhood beaverton oregon father owned farm...   \n",
              "1  nisei male born june seattle washington grew area seattle japanese american attending university...   \n",
              "2  nisei female born october seattle washington family owned operated successful grocery store prio...   \n",
              "3  nisei female born july boyle height california early age went live family friend father passed a...   \n",
              "4  sansei male born march torrance california grew los angeles area father sold insurance active lo...   \n",
              "\n",
              "                                                                                          stemmed_text  \\\n",
              "0  nisei femal born may selleck washington spent much childhood beaverton oregon father own farm in...   \n",
              "1  nisei male born june seattl washington grew area seattl japanes american attend univers washingt...   \n",
              "2  nisei femal born octob seattl washington famili own oper success groceri store prior world war i...   \n",
              "3  nisei femal born juli boyl height california earli age went live famili friend father pass away ...   \n",
              "4  sansei male born march torranc california grew lo angel area father sold insur activ lo angel ko...   \n",
              "\n",
              "                                                                                       lemmatized_text  \n",
              "0  nisei female born may selleck washington spent much childhood beaverton oregon father owned farm...  \n",
              "1  nisei male born june seattle washington grew area seattle japanese american attending university...  \n",
              "2  nisei female born october seattle washington family owned operated successful grocery store prio...  \n",
              "3  nisei female born july boyle height california early age went live family friend father passed a...  \n",
              "4  sansei male born march torrance california grew los angeles area father sold insurance active lo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bfc4afb9-aefc-415c-98d7-ac629cb17f6a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>source_text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>stemmed_text</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>361</td>\n",
              "      <td>Kay Aiko Abe</td>\n",
              "      <td>Nisei female. Born May 9, 1927, in Selleck, Washington. Spent much of childhood in Beaverton, Or...</td>\n",
              "      <td>nisei female born may selleck washington spent much childhood beaverton oregon father owned farm...</td>\n",
              "      <td>nisei femal born may selleck washington spent much childhood beaverton oregon father own farm in...</td>\n",
              "      <td>nisei female born may selleck washington spent much childhood beaverton oregon father owned farm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>291</td>\n",
              "      <td>Art Abe</td>\n",
              "      <td>Nisei male. Born June 12, 1921, in Seattle, Washington. Grew up in an area of Seattle with few o...</td>\n",
              "      <td>nisei male born june seattle washington grew area seattle japanese american attending university...</td>\n",
              "      <td>nisei male born june seattl washington grew area seattl japanes american attend univers washingt...</td>\n",
              "      <td>nisei male born june seattle washington grew area seattle japanese american attending university...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>293</td>\n",
              "      <td>Sharon Tanagi Aburano</td>\n",
              "      <td>Nisei female. Born October 31, 1925, in Seattle, Washington. Family owned and operated a success...</td>\n",
              "      <td>nisei female born october seattle washington family owned operated successful grocery store prio...</td>\n",
              "      <td>nisei femal born octob seattl washington famili own oper success groceri store prior world war i...</td>\n",
              "      <td>nisei female born october seattle washington family owned operated successful grocery store prio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>597</td>\n",
              "      <td>Toshiko Aiboshi</td>\n",
              "      <td>Nisei female. Born July 8, 1928, in Boyle Heights, California. At an early age, went to live wit...</td>\n",
              "      <td>nisei female born july boyle height california early age went live family friend father passed a...</td>\n",
              "      <td>nisei femal born juli boyl height california earli age went live famili friend father pass away ...</td>\n",
              "      <td>nisei female born july boyle height california early age went live family friend father passed a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1014</td>\n",
              "      <td>Douglas L. Aihara</td>\n",
              "      <td>Sansei male. Born March 15, 1950, in Torrance, California. Grew up in the Los Angeles area, wher...</td>\n",
              "      <td>sansei male born march torrance california grew los angeles area father sold insurance active lo...</td>\n",
              "      <td>sansei male born march torranc california grew lo angel area father sold insur activ lo angel ko...</td>\n",
              "      <td>sansei male born march torrance california grew los angeles area father sold insurance active lo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfc4afb9-aefc-415c-98d7-ac629cb17f6a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bfc4afb9-aefc-415c-98d7-ac629cb17f6a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bfc4afb9-aefc-415c-98d7-ac629cb17f6a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-746d8cee-5748-48a7-8848-4d7e0dd483ad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-746d8cee-5748-48a7-8848-4d7e0dd483ad')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-746d8cee-5748-48a7-8848-4d7e0dd483ad button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(out_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 307,\n        \"min\": 291,\n        \"max\": 1014,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          291,\n          1014,\n          293\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Art Abe\",\n          \"Douglas L. Aihara\",\n          \"Sharon Tanagi Aburano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Nisei male. Born June 12, 1921, in Seattle, Washington. Grew up in an area of Seattle with few other Japanese Americans, and was attending the University of Washington when Japan bombed Pearl Harbor. Removed to the Puyallup Assembly Center, Washington, and the Minidoka concentration camp, Idaho. Left Minidoka several times on temporary work leave to work on farms in the area. Suffered tragic loss in camp when father got lost outside collecting wood and perished in the elements. Volunteered for the army and served in the signal corps, eventually working for the Civil Aeronautics Administration and Boeing. Also one of the early members of the Seattle Nisei Veterans Committee.\",\n          \"Sansei male. Born March 15, 1950, in Torrance, California. Grew up in the Los Angeles area, where father sold insurance. Active with Los Angeles' Koyasan Buddhist Temple's Boy Scout troop and performed with its drum and bugle corps. Attended UCLA and joined the staff of Gidra, an Asian American monthly newspaper-magazine started by a group of students. Eventually took over father's insurance business.\",\n          \"Nisei female. Born October 31, 1925, in Seattle, Washington. Family owned and operated a successful grocery store prior to World War II. After the bombing of Pearl Harbor, removed to the Puyallup Assembly Center, Washington, and the Minidoka concentration camp, Idaho. Left camp in 1944 to attend St. Mary's School of Nursing in Rochester, Minnesota. Worked in the medical field in Minnesota and Seattle before eventually pursuing a career in education.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"nisei male born june seattle washington grew area seattle japanese american attending university washington japan bombed pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left minidoka several time temporary work leave work farm area suffered tragic loss camp father got lost outside collecting wood perished element volunteered army served signal corp eventually working civil aeronautics administration boeing also one early member seattle nisei veteran committee\",\n          \"sansei male born march torrance california grew los angeles area father sold insurance active los angeles koyasan buddhist temple boy scout troop performed drum bugle corp attended ucla joined staff gidra asian american monthly newspaper magazine started group student eventually took father insurance business\",\n          \"nisei female born october seattle washington family owned operated successful grocery store prior world war ii bombing pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left camp attend st mary school nursing rochester minnesota worked medical field minnesota seattle eventually pursuing career education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stemmed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"nisei male born june seattl washington grew area seattl japanes american attend univers washington japan bomb pearl harbor remov puyallup assembl center washington minidoka concentr camp idaho left minidoka sever time temporari work leav work farm area suffer tragic loss camp father got lost outsid collect wood perish element volunt armi serv signal corp eventu work civil aeronaut administr boe also one earli member seattl nisei veteran committe\",\n          \"sansei male born march torranc california grew lo angel area father sold insur activ lo angel koyasan buddhist templ boy scout troop perform drum bugl corp attend ucla join staff gidra asian american monthli newspap magazin start group student eventu took father insur busi\",\n          \"nisei femal born octob seattl washington famili own oper success groceri store prior world war ii bomb pearl harbor remov puyallup assembl center washington minidoka concentr camp idaho left camp attend st mari school nurs rochest minnesota work medic field minnesota seattl eventu pursu career educ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"nisei male born june seattle washington grew area seattle japanese american attending university washington japan bombed pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left minidoka several time temporary work leave work farm area suffered tragic loss camp father got lost outside collecting wood perished element volunteered army served signal corp eventually working civil aeronautics administration boeing also one early member seattle nisei veteran committee\",\n          \"sansei male born march torrance california grew los angeles area father sold insurance active los angeles koyasan buddhist temple boy scout troop performed drum bugle corp attended ucla joined staff gidra asian american monthly newspaper magazine started group student eventually took father insurance business\",\n          \"nisei female born october seattle washington family owned operated successful grocery store prior world war ii bombing pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left camp attend st mary school nursing rochester minnesota worked medical field minnesota seattle eventually pursuing career education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup Stanza-only stack & restart (Colab) ===\n",
        "!pip -q uninstall -y numpy || true\n",
        "!pip -q install \"numpy==1.26.4\"\n",
        "\n",
        "# CPU wheel for broad compatibility; remove index-url if you want CUDA/GPU\n",
        "!pip -q install --index-url https://download.pytorch.org/whl/cpu \"torch==2.3.1\"\n",
        "\n",
        "!pip -q install \"stanza==1.8.2\" \"pandas==2.2.2\" \"tqdm==4.66.5\"\n",
        "\n",
        "import stanza\n",
        "# Download all needed processors in one go (includes constituency)\n",
        "stanza.download('en', processors='tokenize,pos,lemma,depparse,ner,constituency', verbose=False)\n",
        "\n",
        "# Force a clean restart so pinned wheels load properly\n",
        "import os, time\n",
        "print(\"Restarting runtime to finalize installations...\")\n",
        "time.sleep(1)\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evDhHcKZLqdN",
        "outputId": "8ec9072d-a9c4-45b9-a74e-8745c920428e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchvision 0.23.0+cpu requires torch==2.8.0, but you have torch 2.3.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRestarting runtime to finalize installations...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9a167a47fc4f4e2d83e7f441df72a94e",
            "33143a58e40942898a4f69f7870132e2",
            "4028a36450be484a9b3571edc20a631a",
            "cdc38f5f74fc4c23a680e6ad38c4b263",
            "e796cdad3b004112828aea4840481e7a",
            "914c696deb2d419db24db030eb935415",
            "839ab050c3fa46e0b17c2eb95772857d",
            "317c2f8336074823895fbd1d48693750",
            "dcd2bec1ccda4d258517a7e50c2e77ae",
            "61c14733e7e542b89ab894d6b8d3ceee",
            "fd25753fc96a4bbb89e4fb5138ef44a2"
          ]
        },
        "collapsed": true,
        "outputId": "3926adfd-fb64-48cb-ce5a-de26cc650ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing documents... (POS/DEP/Constituency/NER via Stanza)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1005 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a167a47fc4f4e2d83e7f441df72a94e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence ---\n",
            "nisei female born may selleck washington spent much childhood beaverton oregon father owned farm influenced early age parent conversion christianity world war ii removed portland assembly center oregon minidoka concentration camp idaho war worked establish successful volunteer program feed homeless seattle washington\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tfemale\n",
            "female\t(nsubj)\t-->\tspent\n",
            "born\t(acl)\t-->\tfemale\n",
            "may\t(aux)\t-->\tselleck\n",
            "selleck\t(compound)\t-->\twashington\n",
            "washington\t(obj)\t-->\tborn\n",
            "spent\t(root)\t-->\tROOT\n",
            "much\t(amod)\t-->\tfarm\n",
            "childhood\t(compound)\t-->\tfather\n",
            "beaverton\t(compound)\t-->\tfather\n",
            "oregon\t(compound)\t-->\tfather\n",
            "father\t(compound)\t-->\tfarm\n",
            "owned\t(amod)\t-->\tfarm\n",
            "farm\t(obj)\t-->\tspent\n",
            "influenced\t(amod)\t-->\tfarm\n",
            "early\t(amod)\t-->\tage\n",
            "age\t(compound)\t-->\tconversion\n",
            "parent\t(compound)\t-->\tconversion\n",
            "conversion\t(compound)\t-->\twar\n",
            "christianity\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(parataxis)\t-->\tspent\n",
            "portland\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\twar\n",
            "oregon\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\twar\n",
            "camp\t(compound)\t-->\twar\n",
            "idaho\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tworked\n",
            "worked\t(parataxis)\t-->\tremoved\n",
            "establish\t(xcomp)\t-->\tworked\n",
            "successful\t(amod)\t-->\twashington\n",
            "volunteer\t(compound)\t-->\tprogram\n",
            "program\t(compound)\t-->\tfeed\n",
            "feed\t(compound)\t-->\twashington\n",
            "homeless\t(amod)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(obj)\t-->\testablish\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born june seattle washington grew area seattle japanese american attending university washington japan bombed pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left minidoka several time temporary work leave work farm area suffered tragic loss camp father got lost outside collecting wood perished element volunteered army served signal corp eventually working civil aeronautics administration boeing also one early member seattle nisei veteran committee\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\twashington\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\twashington\n",
            "june\t(compound)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "area\t(compound)\t-->\tseattle\n",
            "seattle\t(compound)\t-->\tharbor\n",
            "japanese\t(amod)\t-->\tharbor\n",
            "american\t(amod)\t-->\tharbor\n",
            "attending\t(amod)\t-->\tharbor\n",
            "university\t(compound)\t-->\tjapan\n",
            "washington\t(compound)\t-->\tjapan\n",
            "japan\t(compound)\t-->\tharbor\n",
            "bombed\t(amod)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(obj)\t-->\tgrew\n",
            "removed\t(acl)\t-->\tharbor\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tidaho\n",
            "washington\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(obj)\t-->\tremoved\n",
            "left\t(amod)\t-->\tminidoka\n",
            "minidoka\t(obj)\t-->\tleave\n",
            "several\t(amod)\t-->\ttime\n",
            "time\t(obl:tmod)\t-->\tleave\n",
            "temporary\t(amod)\t-->\twork\n",
            "work\t(nsubj)\t-->\tleave\n",
            "leave\t(parataxis)\t-->\tsuffered\n",
            "work\t(compound)\t-->\tfarm\n",
            "farm\t(compound)\t-->\tarea\n",
            "area\t(nsubj)\t-->\tsuffered\n",
            "suffered\t(parataxis)\t-->\tgrew\n",
            "tragic\t(amod)\t-->\tfather\n",
            "loss\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tfather\n",
            "father\t(nsubj:pass)\t-->\tlost\n",
            "got\t(aux:pass)\t-->\tlost\n",
            "lost\t(xcomp)\t-->\tsuffered\n",
            "outside\t(advmod)\t-->\tlost\n",
            "collecting\t(advcl)\t-->\tlost\n",
            "wood\t(compound)\t-->\tperished\n",
            "perished\t(amod)\t-->\telement\n",
            "element\t(obj)\t-->\tcollecting\n",
            "volunteered\t(amod)\t-->\tcorp\n",
            "army\t(compound)\t-->\tserved\n",
            "served\t(amod)\t-->\tcorp\n",
            "signal\t(compound)\t-->\tcorp\n",
            "corp\t(obj)\t-->\tcollecting\n",
            "eventually\t(advmod)\t-->\tworking\n",
            "working\t(advcl)\t-->\tserved\n",
            "civil\t(amod)\t-->\tadministration\n",
            "aeronautics\t(compound)\t-->\tadministration\n",
            "administration\t(compound)\t-->\tboeing\n",
            "boeing\t(obj)\t-->\tworking\n",
            "also\t(advmod)\t-->\tcommittee\n",
            "one\t(nummod)\t-->\tcommittee\n",
            "early\t(amod)\t-->\tmember\n",
            "member\t(compound)\t-->\tcommittee\n",
            "seattle\t(compound)\t-->\tcommittee\n",
            "nisei\t(compound)\t-->\tcommittee\n",
            "veteran\t(compound)\t-->\tcommittee\n",
            "committee\t(appos)\t-->\tboeing\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born october seattle washington family owned operated successful grocery store prior world war ii bombing pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left camp attend st mary school nursing rochester minnesota worked medical field minnesota seattle eventually pursuing career education\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tharbor\n",
            "female\t(amod)\t-->\tharbor\n",
            "born\t(amod)\t-->\tharbor\n",
            "october\t(compound)\t-->\tharbor\n",
            "seattle\t(compound)\t-->\tharbor\n",
            "washington\t(compound)\t-->\towned\n",
            "family\t(obl:npmod)\t-->\towned\n",
            "owned\t(amod)\t-->\tharbor\n",
            "operated\t(amod)\t-->\tharbor\n",
            "successful\t(amod)\t-->\tharbor\n",
            "grocery\t(compound)\t-->\tstore\n",
            "store\t(compound)\t-->\tharbor\n",
            "prior\t(amod)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tharbor\n",
            "ii\t(compound)\t-->\twar\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(nsubj)\t-->\tleft\n",
            "removed\t(acl)\t-->\tharbor\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tidaho\n",
            "washington\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tidaho\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(obj)\t-->\tremoved\n",
            "left\t(root)\t-->\tROOT\n",
            "camp\t(obj)\t-->\tleft\n",
            "attend\t(xcomp)\t-->\tleft\n",
            "st\t(compound)\t-->\tschool\n",
            "mary\t(compound)\t-->\tschool\n",
            "school\t(compound)\t-->\tminnesota\n",
            "nursing\t(compound)\t-->\tminnesota\n",
            "rochester\t(compound)\t-->\tminnesota\n",
            "minnesota\t(nsubj)\t-->\tworked\n",
            "worked\t(ccomp)\t-->\tattend\n",
            "medical\t(amod)\t-->\tfield\n",
            "field\t(compound)\t-->\tseattle\n",
            "minnesota\t(compound)\t-->\tseattle\n",
            "seattle\t(obj)\t-->\tworked\n",
            "eventually\t(advmod)\t-->\tpursuing\n",
            "pursuing\t(advcl)\t-->\tworked\n",
            "career\t(compound)\t-->\teducation\n",
            "education\t(obj)\t-->\tpursuing\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born july boyle height california early age went live family friend father passed away mother contracted tuburculosis world war ii removed santa anita assembly center california granada amache concentration camp colorado camp mother relapsed toshiko went live family friend war returned california\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tage\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tage\n",
            "july\t(compound)\t-->\theight\n",
            "boyle\t(compound)\t-->\theight\n",
            "height\t(compound)\t-->\tage\n",
            "california\t(compound)\t-->\tage\n",
            "early\t(amod)\t-->\tage\n",
            "age\t(nsubj)\t-->\twent\n",
            "went\t(root)\t-->\tROOT\n",
            "live\t(amod)\t-->\tfather\n",
            "family\t(compound)\t-->\tfriend\n",
            "friend\t(compound)\t-->\tfather\n",
            "father\t(obj)\t-->\twent\n",
            "passed\t(acl)\t-->\tfather\n",
            "away\t(advmod)\t-->\tpassed\n",
            "mother\t(compound)\t-->\tcontracted\n",
            "contracted\t(amod)\t-->\twar\n",
            "tuburculosis\t(compound)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(nsubj)\t-->\tremoved\n",
            "removed\t(parataxis)\t-->\twent\n",
            "santa\t(compound)\t-->\tcenter\n",
            "anita\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tgranada\n",
            "granada\t(compound)\t-->\tmother\n",
            "amache\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tmother\n",
            "colorado\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tmother\n",
            "mother\t(nsubj)\t-->\twent\n",
            "relapsed\t(amod)\t-->\ttoshiko\n",
            "toshiko\t(nsubj)\t-->\twent\n",
            "went\t(parataxis)\t-->\twent\n",
            "live\t(amod)\t-->\twar\n",
            "family\t(compound)\t-->\tfriend\n",
            "friend\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\treturned\n",
            "returned\t(conj)\t-->\twent\n",
            "california\t(obj)\t-->\treturned\n",
            "\n",
            "--- Sentence ---\n",
            "sansei male born march torrance california grew los angeles area father sold insurance active los angeles koyasan buddhist temple boy scout troop performed drum bugle corp attended ucla joined staff gidra asian american monthly newspaper magazine started group student eventually took father insurance business\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "sansei\t(compound)\t-->\tcalifornia\n",
            "male\t(compound)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "march\t(compound)\t-->\tcalifornia\n",
            "torrance\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tarea\n",
            "area\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tsold\n",
            "sold\t(ccomp)\t-->\tgrew\n",
            "insurance\t(compound)\t-->\tangeles\n",
            "active\t(amod)\t-->\tangeles\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\ttroop\n",
            "koyasan\t(compound)\t-->\tangeles\n",
            "buddhist\t(amod)\t-->\tangeles\n",
            "temple\t(compound)\t-->\tboy\n",
            "boy\t(compound)\t-->\ttroop\n",
            "scout\t(compound)\t-->\ttroop\n",
            "troop\t(obj)\t-->\tsold\n",
            "performed\t(acl)\t-->\ttroop\n",
            "drum\t(compound)\t-->\tbugle\n",
            "bugle\t(compound)\t-->\tcorp\n",
            "corp\t(obj)\t-->\tperformed\n",
            "attended\t(acl)\t-->\tcorp\n",
            "ucla\t(nsubj)\t-->\tjoined\n",
            "joined\t(parataxis)\t-->\tsold\n",
            "staff\t(compound)\t-->\tgidra\n",
            "gidra\t(compound)\t-->\tasian\n",
            "asian\t(amod)\t-->\tmagazine\n",
            "american\t(amod)\t-->\tmagazine\n",
            "monthly\t(amod)\t-->\tmagazine\n",
            "newspaper\t(compound)\t-->\tmagazine\n",
            "magazine\t(obj)\t-->\tjoined\n",
            "started\t(acl)\t-->\tmagazine\n",
            "group\t(compound)\t-->\tstudent\n",
            "student\t(obj)\t-->\tstarted\n",
            "eventually\t(advmod)\t-->\ttook\n",
            "took\t(parataxis)\t-->\tjoined\n",
            "father\t(compound)\t-->\tinsurance\n",
            "insurance\t(compound)\t-->\tbusiness\n",
            "business\t(obj)\t-->\ttook\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born august tacoma washington raised seattle washington family operated grocery store attended washington grammar school garfield high school seattle following bombing pearl harbor father arrested fbi sent missoula internment camp montana family removed puyallup assembly center washington minidoka concentration camp idaho father decided repatriate japan family transported elli island detention station reunite father board repatriation ship s gripsholm transferred crystal city internment camp texas denied entry s gripsholm remained crystal city duration war resettled boyle height neighborhood los angeles\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tstore\n",
            "female\t(amod)\t-->\tstore\n",
            "born\t(amod)\t-->\tstore\n",
            "august\t(compound)\t-->\tstore\n",
            "tacoma\t(compound)\t-->\tstore\n",
            "washington\t(compound)\t-->\tstore\n",
            "raised\t(amod)\t-->\tstore\n",
            "seattle\t(compound)\t-->\tstore\n",
            "washington\t(compound)\t-->\tstore\n",
            "family\t(compound)\t-->\tstore\n",
            "operated\t(amod)\t-->\tstore\n",
            "grocery\t(compound)\t-->\tstore\n",
            "store\t(compound)\t-->\tseattle\n",
            "attended\t(amod)\t-->\tseattle\n",
            "washington\t(compound)\t-->\tschool\n",
            "grammar\t(compound)\t-->\tschool\n",
            "school\t(compound)\t-->\tseattle\n",
            "garfield\t(compound)\t-->\tschool\n",
            "high\t(amod)\t-->\tschool\n",
            "school\t(compound)\t-->\tseattle\n",
            "seattle\t(obj)\t-->\tdecided\n",
            "following\t(case)\t-->\tfather\n",
            "bombing\t(compound)\t-->\tfather\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(compound)\t-->\tfather\n",
            "father\t(nmod)\t-->\tseattle\n",
            "arrested\t(acl)\t-->\tfather\n",
            "fbi\t(obj)\t-->\tsent\n",
            "sent\t(acl)\t-->\tseattle\n",
            "missoula\t(compound)\t-->\tcamp\n",
            "internment\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tmontana\n",
            "montana\t(compound)\t-->\tfamily\n",
            "family\t(obj)\t-->\tsent\n",
            "removed\t(amod)\t-->\tfather\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tfather\n",
            "washington\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tfather\n",
            "idaho\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tdecided\n",
            "decided\t(root)\t-->\tROOT\n",
            "repatriate\t(compound)\t-->\tjapan\n",
            "japan\t(compound)\t-->\tstation\n",
            "family\t(compound)\t-->\ttransported\n",
            "transported\t(amod)\t-->\tstation\n",
            "elli\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\tstation\n",
            "detention\t(compound)\t-->\tstation\n",
            "station\t(obj)\t-->\tdecided\n",
            "reunite\t(ccomp)\t-->\tdecided\n",
            "father\t(compound)\t-->\tship\n",
            "board\t(compound)\t-->\tship\n",
            "repatriation\t(compound)\t-->\tship\n",
            "ship\t(nmod:poss)\t-->\tgripsholm\n",
            "s\t(case)\t-->\tship\n",
            "gripsholm\t(nsubj)\t-->\ttransferred\n",
            "transferred\t(ccomp)\t-->\treunite\n",
            "crystal\t(compound)\t-->\tcity\n",
            "city\t(compound)\t-->\tcamp\n",
            "internment\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\ttexas\n",
            "texas\t(obj)\t-->\ttransferred\n",
            "denied\t(acl)\t-->\ttexas\n",
            "entry\t(nmod:poss)\t-->\tgripsholm\n",
            "s\t(case)\t-->\tentry\n",
            "gripsholm\t(nsubj)\t-->\tremained\n",
            "remained\t(parataxis)\t-->\ttransferred\n",
            "crystal\t(amod)\t-->\tduration\n",
            "city\t(compound)\t-->\tduration\n",
            "duration\t(compound)\t-->\twar\n",
            "war\t(xcomp)\t-->\tremained\n",
            "resettled\t(amod)\t-->\tangeles\n",
            "boyle\t(compound)\t-->\theight\n",
            "height\t(compound)\t-->\tneighborhood\n",
            "neighborhood\t(compound)\t-->\tangeles\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(xcomp)\t-->\tremained\n",
            "\n",
            "--- Sentence ---\n",
            "session elaine kim led panel elaine reiko akagi friend ann fujii lindwall arlene oki karen yoshitomi bill tashima remembrance conversation akagi elaine reiko akagi devoted life jacl teaching child special education need love animal active detroit jr jacl early led detroit chapter action vincent chin murder detroit moving seattle akagi became chapter president akagi one chapter leader initiating teacher workshop instruct teacher method teach lesson ja wwii classroom instill critical thinking student apply lesson current day issue akagi endowed chapter scholarship primary purpose encouraging student color go field special education akagi firm believer child education enhanced teacher look like\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "session\t(root)\t-->\tROOT\n",
            "elaine\t(flat)\t-->\tsession\n",
            "kim\t(flat)\t-->\tsession\n",
            "led\t(flat)\t-->\tsession\n",
            "panel\t(flat)\t-->\tsession\n",
            "elaine\t(flat)\t-->\tsession\n",
            "reiko\t(flat)\t-->\tsession\n",
            "akagi\t(flat)\t-->\tsession\n",
            "friend\t(flat)\t-->\tsession\n",
            "ann\t(flat)\t-->\tsession\n",
            "fujii\t(flat)\t-->\tsession\n",
            "lindwall\t(flat)\t-->\tsession\n",
            "arlene\t(flat)\t-->\tsession\n",
            "oki\t(flat)\t-->\tsession\n",
            "karen\t(flat)\t-->\tsession\n",
            "yoshitomi\t(flat)\t-->\tsession\n",
            "bill\t(flat)\t-->\tsession\n",
            "tashima\t(flat)\t-->\tsession\n",
            "remembrance\t(flat)\t-->\tsession\n",
            "conversation\t(flat)\t-->\tsession\n",
            "akagi\t(flat)\t-->\tsession\n",
            "elaine\t(flat)\t-->\tconversation\n",
            "reiko\t(flat)\t-->\tsession\n",
            "akagi\t(flat)\t-->\tsession\n",
            "devoted\t(parataxis)\t-->\tsession\n",
            "life\t(obj)\t-->\tdevoted\n",
            "jacl\t(flat)\t-->\tsession\n",
            "teaching\t(compound)\t-->\tchild\n",
            "child\t(flat)\t-->\tsession\n",
            "special\t(amod)\t-->\teducation\n",
            "education\t(conj)\t-->\tsession\n",
            "need\t(conj)\t-->\tsession\n",
            "love\t(conj)\t-->\tsession\n",
            "animal\t(conj)\t-->\tsession\n",
            "active\t(amod)\t-->\tdetroit\n",
            "detroit\t(conj)\t-->\tsession\n",
            "jr\t(flat)\t-->\tdetroit\n",
            "jacl\t(flat)\t-->\tdetroit\n",
            "early\t(compound)\t-->\tled\n",
            "led\t(amod)\t-->\tchapter\n",
            "detroit\t(compound)\t-->\tchapter\n",
            "chapter\t(conj)\t-->\tsession\n",
            "action\t(appos)\t-->\tchapter\n",
            "vincent\t(appos)\t-->\tchapter\n",
            "chin\t(flat)\t-->\tvincent\n",
            "murder\t(flat)\t-->\tvincent\n",
            "detroit\t(compound)\t-->\tmoving\n",
            "moving\t(compound)\t-->\tseattle\n",
            "seattle\t(nsubj)\t-->\tbecame\n",
            "akagi\t(flat)\t-->\tseattle\n",
            "became\t(parataxis)\t-->\tsession\n",
            "chapter\t(obj)\t-->\tbecame\n",
            "president\t(flat)\t-->\tchapter\n",
            "akagi\t(flat)\t-->\tchapter\n",
            "one\t(flat)\t-->\tchapter\n",
            "chapter\t(compound)\t-->\tleader\n",
            "leader\t(appos)\t-->\tchapter\n",
            "initiating\t(compound)\t-->\tteacher\n",
            "teacher\t(flat)\t-->\tchapter\n",
            "workshop\t(compound)\t-->\tmethod\n",
            "instruct\t(compound)\t-->\tmethod\n",
            "teacher\t(compound)\t-->\tmethod\n",
            "method\t(conj)\t-->\tsession\n",
            "teach\t(appos)\t-->\tmethod\n",
            "lesson\t(appos)\t-->\tmethod\n",
            "ja\t(flat)\t-->\tlesson\n",
            "wwii\t(flat)\t-->\tlesson\n",
            "classroom\t(flat)\t-->\tlesson\n",
            "instill\t(flat)\t-->\tthinking\n",
            "critical\t(amod)\t-->\tthinking\n",
            "thinking\t(compound)\t-->\tstudent\n",
            "student\t(conj)\t-->\tsession\n",
            "apply\t(conj)\t-->\tsession\n",
            "lesson\t(appos)\t-->\tstudent\n",
            "current\t(amod)\t-->\tissue\n",
            "day\t(compound)\t-->\tissue\n",
            "issue\t(conj)\t-->\tsession\n",
            "akagi\t(flat)\t-->\tsession\n",
            "endowed\t(amod)\t-->\tchapter\n",
            "chapter\t(conj)\t-->\tsession\n",
            "scholarship\t(conj)\t-->\tsession\n",
            "primary\t(amod)\t-->\tpurpose\n",
            "purpose\t(appos)\t-->\tchapter\n",
            "encouraging\t(parataxis)\t-->\tsession\n",
            "student\t(compound)\t-->\tcolor\n",
            "color\t(obj)\t-->\tencouraging\n",
            "go\t(compound)\t-->\tfield\n",
            "field\t(obj)\t-->\tencouraging\n",
            "special\t(amod)\t-->\teducation\n",
            "education\t(appos)\t-->\tfield\n",
            "akagi\t(appos)\t-->\teducation\n",
            "firm\t(compound)\t-->\teducation\n",
            "believer\t(compound)\t-->\teducation\n",
            "child\t(compound)\t-->\teducation\n",
            "education\t(nsubj)\t-->\tlook\n",
            "enhanced\t(amod)\t-->\tteacher\n",
            "teacher\t(nsubj)\t-->\tlook\n",
            "look\t(parataxis)\t-->\tsession\n",
            "like\t(discourse)\t-->\tlook\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born july lindsay california following bombing pearl harbor moved family idaho voluntary evacuation worked harvesting sugar beet sugar company volunteered army world war ii served nd field artillery battalion present liberation one dachau concentration camp sub camp war settled salt lake city utah\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tcalifornia\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "july\t(compound)\t-->\tcalifornia\n",
            "lindsay\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tmoved\n",
            "following\t(case)\t-->\tharbor\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(nmod)\t-->\tcalifornia\n",
            "moved\t(root)\t-->\tROOT\n",
            "family\t(compound)\t-->\tidaho\n",
            "idaho\t(compound)\t-->\tevacuation\n",
            "voluntary\t(amod)\t-->\tevacuation\n",
            "evacuation\t(nsubj)\t-->\tworked\n",
            "worked\t(ccomp)\t-->\tmoved\n",
            "harvesting\t(xcomp)\t-->\tworked\n",
            "sugar\t(compound)\t-->\tbeet\n",
            "beet\t(compound)\t-->\tcompany\n",
            "sugar\t(compound)\t-->\tcompany\n",
            "company\t(obj)\t-->\tworked\n",
            "volunteered\t(amod)\t-->\twar\n",
            "army\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tliberation\n",
            "ii\t(dep)\t-->\twar\n",
            "served\t(amod)\t-->\tliberation\n",
            "nd\t(compound)\t-->\tfield\n",
            "field\t(compound)\t-->\tbattalion\n",
            "artillery\t(compound)\t-->\tbattalion\n",
            "battalion\t(compound)\t-->\tliberation\n",
            "present\t(amod)\t-->\tliberation\n",
            "liberation\t(obj)\t-->\tworked\n",
            "one\t(nummod)\t-->\tutah\n",
            "dachau\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcamp\n",
            "sub\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tsettled\n",
            "settled\t(amod)\t-->\tutah\n",
            "salt\t(compound)\t-->\tlake\n",
            "lake\t(compound)\t-->\tutah\n",
            "city\t(compound)\t-->\tutah\n",
            "utah\t(appos)\t-->\tliberation\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born june merced california grew mount eden california removed tanforan assembly center california bombing pearl harbor incarcerated topaz concentration camp utah moved tule lake concentration camp family volunteered move japan tule lake joined pro japan organization created father sokoku kenkyu seinen dan young men association study motherland renounced citizenship expatriated japan parent sibling lived worked japan returned united state author betrayed trust story deported issei american born family wwii published\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tcalifornia\n",
            "male\t(compound)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "june\t(compound)\t-->\tcalifornia\n",
            "merced\t(amod)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "mount\t(compound)\t-->\tcalifornia\n",
            "eden\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tgrew\n",
            "removed\t(acl)\t-->\tcalifornia\n",
            "tanforan\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tbombing\n",
            "california\t(compound)\t-->\tbombing\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(obj)\t-->\tremoved\n",
            "incarcerated\t(amod)\t-->\tcamp\n",
            "topaz\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tmove\n",
            "utah\t(compound)\t-->\tmoved\n",
            "moved\t(amod)\t-->\tmove\n",
            "tule\t(compound)\t-->\tlake\n",
            "lake\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tmove\n",
            "family\t(compound)\t-->\tmove\n",
            "volunteered\t(amod)\t-->\tmove\n",
            "move\t(parataxis)\t-->\tgrew\n",
            "japan\t(compound)\t-->\tlake\n",
            "tule\t(compound)\t-->\tlake\n",
            "lake\t(appos)\t-->\tmove\n",
            "joined\t(acl)\t-->\tmove\n",
            "pro\t(amod)\t-->\torganization\n",
            "japan\t(compound)\t-->\torganization\n",
            "organization\t(obj)\t-->\tjoined\n",
            "created\t(acl)\t-->\torganization\n",
            "father\t(xcomp)\t-->\tcreated\n",
            "sokoku\t(flat)\t-->\tfather\n",
            "kenkyu\t(flat)\t-->\tfather\n",
            "seinen\t(flat)\t-->\tfather\n",
            "dan\t(flat)\t-->\tfather\n",
            "young\t(amod)\t-->\tassociation\n",
            "men\t(compound)\t-->\tassociation\n",
            "association\t(compound)\t-->\tmotherland\n",
            "study\t(compound)\t-->\tmotherland\n",
            "motherland\t(compound)\t-->\tcitizenship\n",
            "renounced\t(amod)\t-->\tcitizenship\n",
            "citizenship\t(obj)\t-->\treturned\n",
            "expatriated\t(amod)\t-->\tsibling\n",
            "japan\t(compound)\t-->\tparent\n",
            "parent\t(compound)\t-->\tsibling\n",
            "sibling\t(nsubj)\t-->\tlived\n",
            "lived\t(acl:relcl)\t-->\tcitizenship\n",
            "worked\t(xcomp)\t-->\tlived\n",
            "japan\t(nsubj)\t-->\tlived\n",
            "returned\t(parataxis)\t-->\tgrew\n",
            "united\t(amod)\t-->\tauthor\n",
            "state\t(compound)\t-->\tauthor\n",
            "author\t(obj)\t-->\treturned\n",
            "betrayed\t(amod)\t-->\tstory\n",
            "trust\t(compound)\t-->\tstory\n",
            "story\t(obj)\t-->\treturned\n",
            "deported\t(amod)\t-->\twwii\n",
            "issei\t(compound)\t-->\tborn\n",
            "american\t(compound)\t-->\tborn\n",
            "born\t(amod)\t-->\twwii\n",
            "family\t(compound)\t-->\twwii\n",
            "wwii\t(obj)\t-->\treturned\n",
            "published\t(acl)\t-->\twwii\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born may eastport idaho spent childhood spokane washington traveled japan family father became ill died attended school japan three year returned spokane worked farm following bombing pearl harbor volunteered military service failed medical examination war took brief trip manzanar concentration camp california camp arkansas minidoka concentration camp idaho war worked painter large sign spokane\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tfather\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tfather\n",
            "may\t(aux)\t-->\tspent\n",
            "eastport\t(compound)\t-->\tidaho\n",
            "idaho\t(compound)\t-->\tfather\n",
            "spent\t(amod)\t-->\tfather\n",
            "childhood\t(compound)\t-->\tidaho\n",
            "spokane\t(compound)\t-->\ttraveled\n",
            "washington\t(compound)\t-->\ttraveled\n",
            "traveled\t(amod)\t-->\tfather\n",
            "japan\t(compound)\t-->\tfather\n",
            "family\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tbecame\n",
            "became\t(root)\t-->\tROOT\n",
            "ill\t(advmod)\t-->\tdied\n",
            "died\t(xcomp)\t-->\tbecame\n",
            "attended\t(amod)\t-->\tjapan\n",
            "school\t(compound)\t-->\tjapan\n",
            "japan\t(nsubj)\t-->\tworked\n",
            "three\t(nummod)\t-->\tyear\n",
            "year\t(obl:npmod)\t-->\treturned\n",
            "returned\t(amod)\t-->\tjapan\n",
            "spokane\t(nsubj)\t-->\tworked\n",
            "worked\t(parataxis)\t-->\tbecame\n",
            "farm\t(obj)\t-->\tworked\n",
            "following\t(case)\t-->\tservice\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(compound)\t-->\tservice\n",
            "volunteered\t(amod)\t-->\tservice\n",
            "military\t(amod)\t-->\tservice\n",
            "service\t(compound)\t-->\twar\n",
            "failed\t(amod)\t-->\twar\n",
            "medical\t(amod)\t-->\texamination\n",
            "examination\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\ttook\n",
            "took\t(parataxis)\t-->\tworked\n",
            "brief\t(amod)\t-->\ttrip\n",
            "trip\t(obj)\t-->\ttook\n",
            "manzanar\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcamp\n",
            "california\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tarkansas\n",
            "arkansas\t(compound)\t-->\tcamp\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\twar\n",
            "idaho\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tworked\n",
            "worked\t(parataxis)\t-->\ttook\n",
            "painter\t(compound)\t-->\tspokane\n",
            "large\t(amod)\t-->\tspokane\n",
            "sign\t(compound)\t-->\tspokane\n",
            "spokane\t(obj)\t-->\tworked\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born april oak grove oregon grew hood river oregon area parent ran farm world war ii removed pinedale assembly center california tule lake concentration camp california minidoka concentration camp idaho volunteered serve army war military service returned hood river area\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(nsubj)\t-->\tgrew\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tgrove\n",
            "april\t(compound)\t-->\tgrove\n",
            "oak\t(compound)\t-->\tgrove\n",
            "grove\t(compound)\t-->\toregon\n",
            "oregon\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "hood\t(compound)\t-->\triver\n",
            "river\t(compound)\t-->\tarea\n",
            "oregon\t(compound)\t-->\tarea\n",
            "area\t(compound)\t-->\tparent\n",
            "parent\t(compound)\t-->\twar\n",
            "ran\t(amod)\t-->\twar\n",
            "farm\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(parataxis)\t-->\tgrew\n",
            "pinedale\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tlake\n",
            "california\t(compound)\t-->\tlake\n",
            "tule\t(compound)\t-->\tlake\n",
            "lake\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tidaho\n",
            "california\t(compound)\t-->\tidaho\n",
            "minidoka\t(compound)\t-->\tidaho\n",
            "concentration\t(compound)\t-->\tidaho\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(nsubj)\t-->\tvolunteered\n",
            "volunteered\t(parataxis)\t-->\tremoved\n",
            "serve\t(xcomp)\t-->\tvolunteered\n",
            "army\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tservice\n",
            "military\t(amod)\t-->\tservice\n",
            "service\t(obj)\t-->\tserve\n",
            "returned\t(amod)\t-->\tarea\n",
            "hood\t(compound)\t-->\tarea\n",
            "river\t(compound)\t-->\tarea\n",
            "area\t(obj)\t-->\tserve\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born january bellevue washington grew bellevue removed pinedale assembly center tule lake concentration camp california world war ii leaving camp returned bellevue\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\twashington\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\twashington\n",
            "january\t(compound)\t-->\twashington\n",
            "bellevue\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "bellevue\t(obj)\t-->\tgrew\n",
            "removed\t(amod)\t-->\tbellevue\n",
            "pinedale\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tlake\n",
            "tule\t(compound)\t-->\tlake\n",
            "lake\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\twar\n",
            "california\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tleaving\n",
            "ii\t(compound)\t-->\twar\n",
            "leaving\t(compound)\t-->\tbellevue\n",
            "camp\t(compound)\t-->\tbellevue\n",
            "returned\t(amod)\t-->\tbellevue\n",
            "bellevue\t(parataxis)\t-->\tgrew\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born may world war ii served interpreter military intelligence service\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tmale\n",
            "male\t(compound)\t-->\tborn\n",
            "born\t(csubj)\t-->\tserved\n",
            "may\t(aux)\t-->\tserved\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tserved\n",
            "ii\t(appos)\t-->\twar\n",
            "served\t(root)\t-->\tROOT\n",
            "interpreter\t(compound)\t-->\tservice\n",
            "military\t(amod)\t-->\tintelligence\n",
            "intelligence\t(compound)\t-->\tservice\n",
            "service\t(obj)\t-->\tserved\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born july world war ii served military intelligence service\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\twar\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tworld\n",
            "july\t(amod)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tserved\n",
            "ii\t(appos)\t-->\twar\n",
            "served\t(root)\t-->\tROOT\n",
            "military\t(amod)\t-->\tservice\n",
            "intelligence\t(compound)\t-->\tservice\n",
            "service\t(obj)\t-->\tserved\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born september seattle washington spent prewar childhood seattle nihonmachi incarcerated puyallup assembly center washington minidoka concentration camp idaho refused participate draft imprisoned mcneil island penitentiary washington draft resistance resettled seattle\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\twashington\n",
            "male\t(amod)\t-->\tseptember\n",
            "born\t(amod)\t-->\twashington\n",
            "september\t(compound)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tspent\n",
            "spent\t(advcl)\t-->\trefused\n",
            "prewar\t(amod)\t-->\tnihonmachi\n",
            "childhood\t(compound)\t-->\tnihonmachi\n",
            "seattle\t(compound)\t-->\tnihonmachi\n",
            "nihonmachi\t(compound)\t-->\tidaho\n",
            "incarcerated\t(amod)\t-->\tidaho\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tidaho\n",
            "washington\t(compound)\t-->\tidaho\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tidaho\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(nsubj)\t-->\trefused\n",
            "refused\t(root)\t-->\tROOT\n",
            "participate\t(compound)\t-->\tdraft\n",
            "draft\t(obj)\t-->\trefused\n",
            "imprisoned\t(amod)\t-->\tisland\n",
            "mcneil\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\tresistance\n",
            "penitentiary\t(compound)\t-->\tresistance\n",
            "washington\t(compound)\t-->\tresistance\n",
            "draft\t(compound)\t-->\tresistance\n",
            "resistance\t(obj)\t-->\trefused\n",
            "resettled\t(acl)\t-->\tresistance\n",
            "seattle\t(obj)\t-->\tresettled\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born january seattle washington incarcerated puyallup assembly center washington minidoka concentration camp idaho resisted draft rationale government classified enemy alien therefore obligation serve imprisoned mcneil island penitentiary washington vocal critic japanese american citizen league resettled seattle washington thought model main character john okada boy\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tidaho\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tidaho\n",
            "january\t(compound)\t-->\tidaho\n",
            "seattle\t(compound)\t-->\tidaho\n",
            "washington\t(compound)\t-->\tidaho\n",
            "incarcerated\t(amod)\t-->\tidaho\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tidaho\n",
            "washington\t(compound)\t-->\tcamp\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(root)\t-->\tROOT\n",
            "resisted\t(acl)\t-->\tidaho\n",
            "draft\t(compound)\t-->\trationale\n",
            "rationale\t(compound)\t-->\talien\n",
            "government\t(compound)\t-->\talien\n",
            "classified\t(amod)\t-->\talien\n",
            "enemy\t(compound)\t-->\talien\n",
            "alien\t(obj)\t-->\tresisted\n",
            "therefore\t(advmod)\t-->\tobligation\n",
            "obligation\t(appos)\t-->\talien\n",
            "serve\t(parataxis)\t-->\tidaho\n",
            "imprisoned\t(amod)\t-->\tisland\n",
            "mcneil\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\tpenitentiary\n",
            "penitentiary\t(compound)\t-->\tcritic\n",
            "washington\t(compound)\t-->\tcritic\n",
            "vocal\t(amod)\t-->\tcritic\n",
            "critic\t(compound)\t-->\tboy\n",
            "japanese\t(amod)\t-->\tboy\n",
            "american\t(amod)\t-->\tleague\n",
            "citizen\t(compound)\t-->\tleague\n",
            "league\t(compound)\t-->\tboy\n",
            "resettled\t(amod)\t-->\tboy\n",
            "seattle\t(compound)\t-->\tmodel\n",
            "washington\t(compound)\t-->\tmodel\n",
            "thought\t(amod)\t-->\tmodel\n",
            "model\t(compound)\t-->\tboy\n",
            "main\t(amod)\t-->\tboy\n",
            "character\t(compound)\t-->\tboy\n",
            "john\t(compound)\t-->\tboy\n",
            "okada\t(compound)\t-->\tboy\n",
            "boy\t(obj)\t-->\tserve\n",
            "\n",
            "--- Sentence ---\n",
            "female suquamish filipino descent born january grew squamish reservation near vancouver british columbia canada moved bainbridge island work strawberry farm married raised family\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "female\t(amod)\t-->\tjanuary\n",
            "suquamish\t(compound)\t-->\tdescent\n",
            "filipino\t(amod)\t-->\tdescent\n",
            "descent\t(compound)\t-->\tjanuary\n",
            "born\t(amod)\t-->\tjanuary\n",
            "january\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "squamish\t(compound)\t-->\treservation\n",
            "reservation\t(obj)\t-->\tgrew\n",
            "near\t(case)\t-->\tcolumbia\n",
            "vancouver\t(compound)\t-->\tcolumbia\n",
            "british\t(amod)\t-->\tcolumbia\n",
            "columbia\t(nmod)\t-->\treservation\n",
            "canada\t(appos)\t-->\tcolumbia\n",
            "moved\t(parataxis)\t-->\tgrew\n",
            "bainbridge\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\twork\n",
            "work\t(compound)\t-->\tfarm\n",
            "strawberry\t(compound)\t-->\tfarm\n",
            "farm\t(obj)\t-->\tmoved\n",
            "married\t(acl)\t-->\tfarm\n",
            "raised\t(amod)\t-->\tfamily\n",
            "family\t(obj)\t-->\tmarried\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born january portland oregon grew portland parent ran hotel world war ii removed portland assembly center oregon minidoka concentration camp idaho leaving camp returned portland\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\toregon\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\toregon\n",
            "january\t(compound)\t-->\toregon\n",
            "portland\t(compound)\t-->\toregon\n",
            "oregon\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "portland\t(compound)\t-->\tparent\n",
            "parent\t(nsubj)\t-->\tran\n",
            "ran\t(xcomp)\t-->\tgrew\n",
            "hotel\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\tran\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(parataxis)\t-->\tgrew\n",
            "portland\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tidaho\n",
            "oregon\t(compound)\t-->\tidaho\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tidaho\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(obj)\t-->\tremoved\n",
            "leaving\t(acl)\t-->\tidaho\n",
            "camp\t(nsubj)\t-->\treturned\n",
            "returned\t(parataxis)\t-->\tremoved\n",
            "portland\t(obj)\t-->\treturned\n",
            "\n",
            "--- Sentence ---\n",
            "white male born november seattle washington son reverend emery andrew japanese baptist church minister seattle many year japanese american seattle removed minidoka concentration camp idaho emery brook moved family twin fall idaho minister nikkei camp visited japanese american friend minidoka throughout war year world war ii returned seattle attending bailey gatzert elementary school recent year returned former site minidoka concentration camp\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "white\t(amod)\t-->\tson\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tson\n",
            "november\t(compound)\t-->\tson\n",
            "seattle\t(compound)\t-->\tson\n",
            "washington\t(compound)\t-->\tson\n",
            "son\t(compound)\t-->\tseattle\n",
            "reverend\t(compound)\t-->\tseattle\n",
            "emery\t(compound)\t-->\tson\n",
            "andrew\t(appos)\t-->\tson\n",
            "japanese\t(amod)\t-->\tminister\n",
            "baptist\t(compound)\t-->\tminister\n",
            "church\t(compound)\t-->\tminister\n",
            "minister\t(compound)\t-->\tseattle\n",
            "seattle\t(compound)\t-->\tyear\n",
            "many\t(amod)\t-->\tyear\n",
            "year\t(compound)\t-->\tseattle\n",
            "japanese\t(amod)\t-->\tseattle\n",
            "american\t(amod)\t-->\tseattle\n",
            "seattle\t(nsubj)\t-->\tmoved\n",
            "removed\t(acl)\t-->\tseattle\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tbrook\n",
            "idaho\t(compound)\t-->\tbrook\n",
            "emery\t(compound)\t-->\tbrook\n",
            "brook\t(nsubj)\t-->\tmoved\n",
            "moved\t(root)\t-->\tROOT\n",
            "family\t(compound)\t-->\tminister\n",
            "twin\t(compound)\t-->\tfall\n",
            "fall\t(compound)\t-->\tminister\n",
            "idaho\t(compound)\t-->\tminister\n",
            "minister\t(compound)\t-->\tcamp\n",
            "nikkei\t(compound)\t-->\tcamp\n",
            "camp\t(obj)\t-->\tvisited\n",
            "visited\t(amod)\t-->\tminidoka\n",
            "japanese\t(amod)\t-->\tminidoka\n",
            "american\t(amod)\t-->\tminidoka\n",
            "friend\t(compound)\t-->\tminidoka\n",
            "minidoka\t(obj)\t-->\tmoved\n",
            "throughout\t(case)\t-->\twar\n",
            "war\t(compound)\t-->\tyear\n",
            "year\t(compound)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(obl)\t-->\treturned\n",
            "ii\t(nsubj)\t-->\treturned\n",
            "returned\t(parataxis)\t-->\tmoved\n",
            "seattle\t(obj)\t-->\treturned\n",
            "attending\t(advcl)\t-->\treturned\n",
            "bailey\t(compound)\t-->\tschool\n",
            "gatzert\t(compound)\t-->\tschool\n",
            "elementary\t(amod)\t-->\tschool\n",
            "school\t(compound)\t-->\tyear\n",
            "recent\t(amod)\t-->\tyear\n",
            "year\t(obj)\t-->\tattending\n",
            "returned\t(acl)\t-->\tyear\n",
            "former\t(amod)\t-->\tsite\n",
            "site\t(compound)\t-->\tcamp\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(obj)\t-->\treturned\n",
            "\n",
            "--- Sentence ---\n",
            "white female born june seattle washington grew central district seattle mother ran boarding house discusses reaction japanese american peer leave seattle world war ii\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "white\t(amod)\t-->\twashington\n",
            "female\t(amod)\t-->\twashington\n",
            "born\t(amod)\t-->\twashington\n",
            "june\t(compound)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "central\t(amod)\t-->\tdistrict\n",
            "district\t(compound)\t-->\tmother\n",
            "seattle\t(compound)\t-->\tmother\n",
            "mother\t(nsubj)\t-->\tran\n",
            "ran\t(ccomp)\t-->\tgrew\n",
            "boarding\t(amod)\t-->\thouse\n",
            "house\t(obj)\t-->\tran\n",
            "discusses\t(parataxis)\t-->\tgrew\n",
            "reaction\t(compound)\t-->\tjapanese\n",
            "japanese\t(amod)\t-->\tpeer\n",
            "american\t(amod)\t-->\tpeer\n",
            "peer\t(nsubj)\t-->\tleave\n",
            "leave\t(ccomp)\t-->\tdiscusses\n",
            "seattle\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\tleave\n",
            "ii\t(obj)\t-->\tleave\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born january kealakekua hawai father congregational preacher world war ii served th battalion europe active hawai democratic party campaign statehood\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tjanuary\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tjanuary\n",
            "january\t(compound)\t-->\tpreacher\n",
            "kealakekua\t(compound)\t-->\tfather\n",
            "hawai\t(compound)\t-->\tfather\n",
            "father\t(compound)\t-->\tpreacher\n",
            "congregational\t(amod)\t-->\twar\n",
            "preacher\t(compound)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tserved\n",
            "ii\t(flat)\t-->\twar\n",
            "served\t(root)\t-->\tROOT\n",
            "th\t(det)\t-->\tstatehood\n",
            "battalion\t(compound)\t-->\teurope\n",
            "europe\t(compound)\t-->\tstatehood\n",
            "active\t(amod)\t-->\tstatehood\n",
            "hawai\t(compound)\t-->\tstatehood\n",
            "democratic\t(amod)\t-->\tparty\n",
            "party\t(compound)\t-->\tstatehood\n",
            "campaign\t(compound)\t-->\tstatehood\n",
            "statehood\t(obj)\t-->\tserved\n",
            "\n",
            "--- Sentence ---\n",
            "sansei female born october san francisco california world war ii removed granada amache concentration camp colorado family briefly left camp utah voluntarily entering topaz concentration camp utah mother pregnant leaving camp returned san francisco resumed elementary school\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "sansei\t(compound)\t-->\tfamily\n",
            "female\t(amod)\t-->\tfamily\n",
            "born\t(amod)\t-->\tfamily\n",
            "october\t(compound)\t-->\twar\n",
            "san\t(compound)\t-->\tworld\n",
            "francisco\t(flat)\t-->\tsan\n",
            "california\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\tfamily\n",
            "war\t(compound)\t-->\tfamily\n",
            "ii\t(compound)\t-->\tfamily\n",
            "removed\t(amod)\t-->\tfamily\n",
            "granada\t(compound)\t-->\tconcentration\n",
            "amache\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tfamily\n",
            "camp\t(compound)\t-->\tfamily\n",
            "colorado\t(compound)\t-->\tfamily\n",
            "family\t(nsubj)\t-->\tleft\n",
            "briefly\t(advmod)\t-->\tleft\n",
            "left\t(root)\t-->\tROOT\n",
            "camp\t(compound)\t-->\tutah\n",
            "utah\t(obj)\t-->\tleft\n",
            "voluntarily\t(advmod)\t-->\tentering\n",
            "entering\t(advcl)\t-->\tleft\n",
            "topaz\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(obj)\t-->\tentering\n",
            "utah\t(compound)\t-->\tmother\n",
            "mother\t(obj)\t-->\tentering\n",
            "pregnant\t(amod)\t-->\tmother\n",
            "leaving\t(acl)\t-->\tpregnant\n",
            "camp\t(obj)\t-->\tleaving\n",
            "returned\t(parataxis)\t-->\tleft\n",
            "san\t(nsubj)\t-->\tresumed\n",
            "francisco\t(flat)\t-->\tsan\n",
            "resumed\t(xcomp)\t-->\treturned\n",
            "elementary\t(amod)\t-->\tschool\n",
            "school\t(obj)\t-->\tresumed\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born july saratoga california grew saratoga father worked gardener large estate bombing pearl harbor father moved family inland reedley attempt avoid mass removal eventually removed poston concentration camp arizona leaving camp returned california graduated stanford university master mechanical engineer went establish prestigous career aerospace industry becoming president lockheed martin missile space\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tfather\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tjuly\n",
            "july\t(compound)\t-->\tcalifornia\n",
            "saratoga\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tfather\n",
            "grew\t(compound)\t-->\tfather\n",
            "saratoga\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tworked\n",
            "worked\t(root)\t-->\tROOT\n",
            "gardener\t(compound)\t-->\tfather\n",
            "large\t(amod)\t-->\testate\n",
            "estate\t(compound)\t-->\tbombing\n",
            "bombing\t(compound)\t-->\tfather\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tmoved\n",
            "moved\t(parataxis)\t-->\tworked\n",
            "family\t(compound)\t-->\tinland\n",
            "inland\t(amod)\t-->\tattempt\n",
            "reedley\t(compound)\t-->\tattempt\n",
            "attempt\t(obj)\t-->\tmoved\n",
            "avoid\t(acl)\t-->\tattempt\n",
            "mass\t(amod)\t-->\tremoval\n",
            "removal\t(obj)\t-->\tavoid\n",
            "eventually\t(advmod)\t-->\tremoved\n",
            "removed\t(acl)\t-->\tattempt\n",
            "poston\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tarizona\n",
            "arizona\t(obj)\t-->\tremoved\n",
            "leaving\t(amod)\t-->\tcalifornia\n",
            "camp\t(compound)\t-->\tcalifornia\n",
            "returned\t(amod)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tremoved\n",
            "graduated\t(amod)\t-->\tengineer\n",
            "stanford\t(compound)\t-->\tuniversity\n",
            "university\t(compound)\t-->\tengineer\n",
            "master\t(compound)\t-->\tengineer\n",
            "mechanical\t(amod)\t-->\tengineer\n",
            "engineer\t(nsubj)\t-->\twent\n",
            "went\t(parataxis)\t-->\tmoved\n",
            "establish\t(xcomp)\t-->\twent\n",
            "prestigous\t(amod)\t-->\tindustry\n",
            "career\t(compound)\t-->\taerospace\n",
            "aerospace\t(compound)\t-->\tindustry\n",
            "industry\t(obj)\t-->\testablish\n",
            "becoming\t(ccomp)\t-->\testablish\n",
            "president\t(compound)\t-->\tspace\n",
            "lockheed\t(compound)\t-->\tmartin\n",
            "martin\t(compound)\t-->\tmissile\n",
            "missile\t(compound)\t-->\tspace\n",
            "space\t(xcomp)\t-->\tbecoming\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born december los angeles california grew gardena parent ran chicken farm bombing pearl harbor moved temporarily live family reedley california removed poston concentration camp arizona visit different camp sakaye met future husband george aratani married minneapolis minnesota george serving military intelligence service leaving camp sakaye george returned los angeles raised family sakaye founding member montebello japanese woman club also one first woman serve board sumitomo bank california\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tcalifornia\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "december\t(compound)\t-->\tcalifornia\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "gardena\t(compound)\t-->\tparent\n",
            "parent\t(compound)\t-->\tharbor\n",
            "ran\t(amod)\t-->\tharbor\n",
            "chicken\t(compound)\t-->\tfarm\n",
            "farm\t(compound)\t-->\tbombing\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(obj)\t-->\tgrew\n",
            "moved\t(acl)\t-->\tharbor\n",
            "temporarily\t(advmod)\t-->\tlive\n",
            "live\t(amod)\t-->\tcalifornia\n",
            "family\t(compound)\t-->\tcalifornia\n",
            "reedley\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tremoved\n",
            "removed\t(parataxis)\t-->\tgrew\n",
            "poston\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tvisit\n",
            "arizona\t(compound)\t-->\tvisit\n",
            "visit\t(obj)\t-->\tremoved\n",
            "different\t(amod)\t-->\tsakaye\n",
            "camp\t(compound)\t-->\tsakaye\n",
            "sakaye\t(nsubj)\t-->\tmet\n",
            "met\t(parataxis)\t-->\tgrew\n",
            "future\t(amod)\t-->\thusband\n",
            "husband\t(compound)\t-->\taratani\n",
            "george\t(compound)\t-->\taratani\n",
            "aratani\t(compound)\t-->\tmarried\n",
            "married\t(amod)\t-->\tservice\n",
            "minneapolis\t(compound)\t-->\tservice\n",
            "minnesota\t(compound)\t-->\tservice\n",
            "george\t(compound)\t-->\tserving\n",
            "serving\t(amod)\t-->\tservice\n",
            "military\t(amod)\t-->\tservice\n",
            "intelligence\t(compound)\t-->\tservice\n",
            "service\t(obj)\t-->\tmet\n",
            "leaving\t(acl)\t-->\tservice\n",
            "camp\t(compound)\t-->\tsakaye\n",
            "sakaye\t(compound)\t-->\tgeorge\n",
            "george\t(nsubj)\t-->\treturned\n",
            "returned\t(acl:relcl)\t-->\tservice\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(obj)\t-->\treturned\n",
            "raised\t(amod)\t-->\tclub\n",
            "family\t(compound)\t-->\tmember\n",
            "sakaye\t(compound)\t-->\tmember\n",
            "founding\t(compound)\t-->\tmember\n",
            "member\t(compound)\t-->\tclub\n",
            "montebello\t(compound)\t-->\tclub\n",
            "japanese\t(amod)\t-->\tclub\n",
            "woman\t(compound)\t-->\tclub\n",
            "club\t(obj)\t-->\treturned\n",
            "also\t(advmod)\t-->\tserve\n",
            "one\t(nummod)\t-->\twoman\n",
            "first\t(amod)\t-->\twoman\n",
            "woman\t(appos)\t-->\tclub\n",
            "serve\t(acl)\t-->\twoman\n",
            "board\t(compound)\t-->\tsumitomo\n",
            "sumitomo\t(compound)\t-->\tcalifornia\n",
            "bank\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tserve\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male world war ii served company part nd regimental combat team japanese american fighting unit one highly decorated military unit history interview discusses experience training camp shelby mississippi fighting europe battle battle lost battalion\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\tunit\n",
            "male\t(amod)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tserved\n",
            "ii\t(compound)\t-->\twar\n",
            "served\t(amod)\t-->\tunit\n",
            "company\t(compound)\t-->\tpart\n",
            "part\t(compound)\t-->\tunit\n",
            "nd\t(compound)\t-->\tteam\n",
            "regimental\t(amod)\t-->\tteam\n",
            "combat\t(compound)\t-->\tteam\n",
            "team\t(compound)\t-->\tunit\n",
            "japanese\t(amod)\t-->\tunit\n",
            "american\t(amod)\t-->\tunit\n",
            "fighting\t(compound)\t-->\tunit\n",
            "unit\t(obj)\t-->\tdiscusses\n",
            "one\t(nummod)\t-->\tinterview\n",
            "highly\t(advmod)\t-->\tdecorated\n",
            "decorated\t(amod)\t-->\tinterview\n",
            "military\t(amod)\t-->\tunit\n",
            "unit\t(compound)\t-->\thistory\n",
            "history\t(compound)\t-->\tinterview\n",
            "interview\t(nsubj)\t-->\tdiscusses\n",
            "discusses\t(root)\t-->\tROOT\n",
            "experience\t(compound)\t-->\tcamp\n",
            "training\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tshelby\n",
            "shelby\t(compound)\t-->\tfighting\n",
            "mississippi\t(compound)\t-->\tfighting\n",
            "fighting\t(compound)\t-->\tbattle\n",
            "europe\t(compound)\t-->\tbattle\n",
            "battle\t(compound)\t-->\tbattle\n",
            "battle\t(compound)\t-->\tbattalion\n",
            "lost\t(amod)\t-->\tbattalion\n",
            "battalion\t(obj)\t-->\tdiscusses\n",
            "\n",
            "--- Sentence ---\n",
            "sansei male born seabrook new jersey parent lived worked following incarceration experience world war ii grew seabrook participating japanese american community activity enrolled west point military academy spent military career armor corp became deputy program executive army civilian remained new jersey raised family became active seabrook chapter japanese american citizen league\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "sansei\t(compound)\t-->\tparent\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tparent\n",
            "seabrook\t(compound)\t-->\tparent\n",
            "new\t(amod)\t-->\tparent\n",
            "jersey\t(compound)\t-->\tparent\n",
            "parent\t(nsubj)\t-->\tworked\n",
            "lived\t(acl)\t-->\tparent\n",
            "worked\t(root)\t-->\tROOT\n",
            "following\t(case)\t-->\texperience\n",
            "incarceration\t(compound)\t-->\texperience\n",
            "experience\t(obl)\t-->\tworked\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tgrew\n",
            "ii\t(appos)\t-->\twar\n",
            "grew\t(parataxis)\t-->\tworked\n",
            "seabrook\t(compound)\t-->\tparticipating\n",
            "participating\t(amod)\t-->\tacademy\n",
            "japanese\t(amod)\t-->\tacademy\n",
            "american\t(amod)\t-->\tactivity\n",
            "community\t(compound)\t-->\tactivity\n",
            "activity\t(compound)\t-->\tacademy\n",
            "enrolled\t(amod)\t-->\tacademy\n",
            "west\t(compound)\t-->\tpoint\n",
            "point\t(compound)\t-->\tacademy\n",
            "military\t(amod)\t-->\tacademy\n",
            "academy\t(obj)\t-->\tgrew\n",
            "spent\t(acl)\t-->\tacademy\n",
            "military\t(amod)\t-->\tcorp\n",
            "career\t(compound)\t-->\tarmor\n",
            "armor\t(compound)\t-->\tcorp\n",
            "corp\t(nsubj)\t-->\tbecame\n",
            "became\t(ccomp)\t-->\tspent\n",
            "deputy\t(compound)\t-->\tprogram\n",
            "program\t(compound)\t-->\tremained\n",
            "executive\t(amod)\t-->\tremained\n",
            "army\t(compound)\t-->\tremained\n",
            "civilian\t(compound)\t-->\tremained\n",
            "remained\t(amod)\t-->\tfamily\n",
            "new\t(amod)\t-->\tfamily\n",
            "jersey\t(compound)\t-->\traised\n",
            "raised\t(amod)\t-->\tfamily\n",
            "family\t(xcomp)\t-->\tbecame\n",
            "became\t(xcomp)\t-->\tbecame\n",
            "active\t(amod)\t-->\tchapter\n",
            "seabrook\t(compound)\t-->\tchapter\n",
            "chapter\t(xcomp)\t-->\tbecame\n",
            "japanese\t(amod)\t-->\tleague\n",
            "american\t(amod)\t-->\tleague\n",
            "citizen\t(compound)\t-->\tleague\n",
            "league\t(conj)\t-->\tchapter\n",
            "\n",
            "--- Sentence ---\n",
            "japanese female born november shizuoka japan attended school world war ii war entered nursing profession became involved number social political cause married kibei nisei moved united state\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "japanese\t(amod)\t-->\twar\n",
            "female\t(amod)\t-->\tjapan\n",
            "born\t(amod)\t-->\tjapan\n",
            "november\t(compound)\t-->\tjapan\n",
            "shizuoka\t(compound)\t-->\tjapan\n",
            "japan\t(compound)\t-->\twar\n",
            "attended\t(amod)\t-->\twar\n",
            "school\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\twar\n",
            "ii\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tentered\n",
            "entered\t(root)\t-->\tROOT\n",
            "nursing\t(compound)\t-->\tprofession\n",
            "profession\t(obj)\t-->\tentered\n",
            "became\t(ccomp)\t-->\tentered\n",
            "involved\t(xcomp)\t-->\tbecame\n",
            "number\t(xcomp)\t-->\tbecame\n",
            "social\t(amod)\t-->\tcause\n",
            "political\t(amod)\t-->\tcause\n",
            "cause\t(appos)\t-->\tnumber\n",
            "married\t(acl)\t-->\tcause\n",
            "kibei\t(compound)\t-->\tnisei\n",
            "nisei\t(nsubj)\t-->\tmoved\n",
            "moved\t(xcomp)\t-->\tmarried\n",
            "united\t(amod)\t-->\tstate\n",
            "state\t(obj)\t-->\tmoved\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born march boyle height neighborhood los angeles california world war ii removed santa anita assembly center california rohwer concentration camp arkansas leaving camp moved family new orleans louisiana remained new orleans thirteen year finishing school establishing career microbiologist eventually returned los angeles\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\twar\n",
            "female\t(amod)\t-->\twar\n",
            "born\t(amod)\t-->\twar\n",
            "march\t(compound)\t-->\tneighborhood\n",
            "boyle\t(compound)\t-->\theight\n",
            "height\t(compound)\t-->\tneighborhood\n",
            "neighborhood\t(compound)\t-->\twar\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\twar\n",
            "california\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(root)\t-->\tROOT\n",
            "santa\t(compound)\t-->\tcenter\n",
            "anita\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tarkansas\n",
            "rohwer\t(amod)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tarkansas\n",
            "arkansas\t(obj)\t-->\tremoved\n",
            "leaving\t(acl)\t-->\tarkansas\n",
            "camp\t(compound)\t-->\tmoved\n",
            "moved\t(amod)\t-->\tarkansas\n",
            "family\t(compound)\t-->\torleans\n",
            "new\t(amod)\t-->\torleans\n",
            "orleans\t(compound)\t-->\tlouisiana\n",
            "louisiana\t(obj)\t-->\tremained\n",
            "remained\t(parataxis)\t-->\tremoved\n",
            "new\t(amod)\t-->\torleans\n",
            "orleans\t(compound)\t-->\tschool\n",
            "thirteen\t(nummod)\t-->\tyear\n",
            "year\t(compound)\t-->\tschool\n",
            "finishing\t(amod)\t-->\tschool\n",
            "school\t(obj)\t-->\tremained\n",
            "establishing\t(acl)\t-->\tschool\n",
            "career\t(compound)\t-->\tmicrobiologist\n",
            "microbiologist\t(nsubj)\t-->\treturned\n",
            "eventually\t(advmod)\t-->\treturned\n",
            "returned\t(parataxis)\t-->\tremoved\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(obj)\t-->\treturned\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born june portland oregon grew portland parent ran hotel grocery business drafted army prior bombing pearl harbor transferred army reserve world war ii removed portland assembly center oregon minidoka concentration camp idaho war returned portland established successful travel agency\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(compound)\t-->\toregon\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\toregon\n",
            "june\t(compound)\t-->\toregon\n",
            "portland\t(compound)\t-->\toregon\n",
            "oregon\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "portland\t(compound)\t-->\tparent\n",
            "parent\t(nsubj)\t-->\tran\n",
            "ran\t(ccomp)\t-->\tgrew\n",
            "hotel\t(compound)\t-->\tbusiness\n",
            "grocery\t(compound)\t-->\tbusiness\n",
            "business\t(compound)\t-->\tarmy\n",
            "drafted\t(amod)\t-->\tarmy\n",
            "army\t(obj)\t-->\tran\n",
            "prior\t(case)\t-->\tharbor\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(obl)\t-->\tran\n",
            "transferred\t(amod)\t-->\tarmy\n",
            "army\t(compound)\t-->\twar\n",
            "reserve\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\ttransferred\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(acl)\t-->\twar\n",
            "portland\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tcamp\n",
            "oregon\t(compound)\t-->\tcamp\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tidaho\n",
            "idaho\t(compound)\t-->\tportland\n",
            "war\t(compound)\t-->\tportland\n",
            "returned\t(amod)\t-->\tportland\n",
            "portland\t(compound)\t-->\tagency\n",
            "established\t(amod)\t-->\tagency\n",
            "successful\t(amod)\t-->\tagency\n",
            "travel\t(compound)\t-->\tagency\n",
            "agency\t(obj)\t-->\tremoved\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born march vashon island washington raised vashon island de moines washington removed two child pinedale assembly center california later transferred tule lake concentration camp california minidoka idaho resettling chicago illinois lived worked chicago eventually returning seattle washington\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\twashington\n",
            "female\t(amod)\t-->\tisland\n",
            "born\t(amod)\t-->\tisland\n",
            "march\t(compound)\t-->\tisland\n",
            "vashon\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\twashington\n",
            "washington\t(compound)\t-->\tisland\n",
            "raised\t(amod)\t-->\tisland\n",
            "vashon\t(compound)\t-->\tisland\n",
            "island\t(compound)\t-->\twashington\n",
            "de\t(compound)\t-->\tmoines\n",
            "moines\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tremoved\n",
            "removed\t(root)\t-->\tROOT\n",
            "two\t(nummod)\t-->\tcalifornia\n",
            "child\t(compound)\t-->\tpinedale\n",
            "pinedale\t(compound)\t-->\tcalifornia\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tremoved\n",
            "later\t(advmod)\t-->\tlived\n",
            "transferred\t(amod)\t-->\tidaho\n",
            "tule\t(compound)\t-->\tlake\n",
            "lake\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tidaho\n",
            "california\t(compound)\t-->\tidaho\n",
            "minidoka\t(compound)\t-->\tidaho\n",
            "idaho\t(nsubj)\t-->\tlived\n",
            "resettling\t(acl)\t-->\tidaho\n",
            "chicago\t(compound)\t-->\tillinois\n",
            "illinois\t(obj)\t-->\tresettling\n",
            "lived\t(parataxis)\t-->\tremoved\n",
            "worked\t(xcomp)\t-->\tlived\n",
            "chicago\t(obj)\t-->\tworked\n",
            "eventually\t(advmod)\t-->\treturning\n",
            "returning\t(advcl)\t-->\tworked\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(obj)\t-->\treturning\n",
            "\n",
            "--- Sentence ---\n",
            "brent seto bill tashima interviewed sarah baker baker four time chapter president two time national jacl vice president local activist community organizer actor dancer director balanced task working full time full time undergraduate graduate student baker organized two community wide gathering support api lgbtq youth family galvanized seattle jacl oppose muslim travel ban planned pnwdc national youth student council activity much\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "brent\t(nsubj)\t-->\tinterviewed\n",
            "seto\t(flat)\t-->\tbrent\n",
            "bill\t(flat)\t-->\tbrent\n",
            "tashima\t(flat)\t-->\tbrent\n",
            "interviewed\t(root)\t-->\tROOT\n",
            "sarah\t(compound)\t-->\tbaker\n",
            "baker\t(compound)\t-->\tbaker\n",
            "baker\t(compound)\t-->\tpresident\n",
            "four\t(nummod)\t-->\ttime\n",
            "time\t(compound)\t-->\tpresident\n",
            "chapter\t(compound)\t-->\tpresident\n",
            "president\t(obj)\t-->\tinterviewed\n",
            "two\t(nummod)\t-->\ttime\n",
            "time\t(compound)\t-->\tpresident\n",
            "national\t(amod)\t-->\tpresident\n",
            "jacl\t(compound)\t-->\tpresident\n",
            "vice\t(compound)\t-->\tpresident\n",
            "president\t(compound)\t-->\tpresident\n",
            "local\t(amod)\t-->\tactor\n",
            "activist\t(compound)\t-->\tactor\n",
            "community\t(compound)\t-->\tactor\n",
            "organizer\t(compound)\t-->\tactor\n",
            "actor\t(compound)\t-->\ttask\n",
            "dancer\t(compound)\t-->\tdirector\n",
            "director\t(compound)\t-->\ttask\n",
            "balanced\t(amod)\t-->\ttask\n",
            "task\t(obj)\t-->\tinterviewed\n",
            "working\t(acl)\t-->\ttask\n",
            "full\t(amod)\t-->\ttime\n",
            "time\t(obj)\t-->\tworking\n",
            "full\t(amod)\t-->\ttime\n",
            "time\t(obl:tmod)\t-->\tworking\n",
            "undergraduate\t(compound)\t-->\tbaker\n",
            "graduate\t(compound)\t-->\tstudent\n",
            "student\t(compound)\t-->\tbaker\n",
            "baker\t(nsubj)\t-->\torganized\n",
            "organized\t(acl)\t-->\tpresident\n",
            "two\t(nummod)\t-->\tjacl\n",
            "community\t(compound)\t-->\tsupport\n",
            "wide\t(amod)\t-->\tsupport\n",
            "gathering\t(compound)\t-->\tsupport\n",
            "support\t(compound)\t-->\tapi\n",
            "api\t(compound)\t-->\tlgbtq\n",
            "lgbtq\t(compound)\t-->\tjacl\n",
            "youth\t(compound)\t-->\tjacl\n",
            "family\t(compound)\t-->\tgalvanized\n",
            "galvanized\t(amod)\t-->\tjacl\n",
            "seattle\t(compound)\t-->\tjacl\n",
            "jacl\t(obj)\t-->\toppose\n",
            "oppose\t(xcomp)\t-->\torganized\n",
            "muslim\t(amod)\t-->\tban\n",
            "travel\t(compound)\t-->\tban\n",
            "ban\t(obj)\t-->\toppose\n",
            "planned\t(amod)\t-->\tactivity\n",
            "pnwdc\t(compound)\t-->\tactivity\n",
            "national\t(amod)\t-->\tactivity\n",
            "youth\t(compound)\t-->\tstudent\n",
            "student\t(compound)\t-->\tactivity\n",
            "council\t(compound)\t-->\tactivity\n",
            "activity\t(obj)\t-->\toppose\n",
            "much\t(advmod)\t-->\toppose\n",
            "\n",
            "--- Sentence ---\n",
            "male japanese french irish descent born october los angeles california child resided child home society orphanage los angeles world war ii transferred manzanar concentration camp child village orphan adopted camp family bishop california eventually became teacher\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "male\t(amod)\t-->\tchild\n",
            "japanese\t(amod)\t-->\tchild\n",
            "french\t(amod)\t-->\tdescent\n",
            "irish\t(amod)\t-->\tdescent\n",
            "descent\t(compound)\t-->\tchild\n",
            "born\t(amod)\t-->\tchild\n",
            "october\t(compound)\t-->\tchild\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tchild\n",
            "california\t(compound)\t-->\tchild\n",
            "child\t(nsubj)\t-->\tbecame\n",
            "resided\t(advcl)\t-->\tbecame\n",
            "child\t(compound)\t-->\tsociety\n",
            "home\t(compound)\t-->\tsociety\n",
            "society\t(nsubj)\t-->\tbecame\n",
            "orphanage\t(compound)\t-->\twar\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tbecame\n",
            "ii\t(dep)\t-->\twar\n",
            "transferred\t(amod)\t-->\tcalifornia\n",
            "manzanar\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tvillage\n",
            "camp\t(compound)\t-->\tvillage\n",
            "child\t(compound)\t-->\tvillage\n",
            "village\t(compound)\t-->\tcalifornia\n",
            "orphan\t(compound)\t-->\tvillage\n",
            "adopted\t(amod)\t-->\tcalifornia\n",
            "camp\t(compound)\t-->\tcalifornia\n",
            "family\t(compound)\t-->\tcalifornia\n",
            "bishop\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tbecame\n",
            "eventually\t(advmod)\t-->\tbecame\n",
            "became\t(root)\t-->\tROOT\n",
            "teacher\t(xcomp)\t-->\tbecame\n",
            "\n",
            "--- Sentence ---\n",
            "elaine kim bill tashima interviewed kathryn bannai kathryn bannai lead counsel gordon hirabayashi coram nobis case february among critical work successfully defeated government effort dismiss hirabayashi case led overturning hirabayashi conviction resisting curfew exclusion order promulgated bannai seattle jacl president pivotal period chapter period chapter expanded work toward wwii japanese american redress aging health issue issei nisei youth programming bannai also forged partnership japanese canadian share wwii experience injustice bannai third woman chapter president one earliest sansei chapter president bannai board first seattle jacl board majority sansei also majority female membership\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "elaine\t(root)\t-->\tROOT\n",
            "kim\t(flat)\t-->\telaine\n",
            "bill\t(compound)\t-->\ttashima\n",
            "tashima\t(flat)\t-->\telaine\n",
            "interviewed\t(acl)\t-->\ttashima\n",
            "kathryn\t(compound)\t-->\tfebruary\n",
            "bannai\t(flat)\t-->\tkathryn\n",
            "kathryn\t(flat)\t-->\tkathryn\n",
            "bannai\t(flat)\t-->\tkathryn\n",
            "lead\t(compound)\t-->\tcounsel\n",
            "counsel\t(compound)\t-->\tfebruary\n",
            "gordon\t(compound)\t-->\tfebruary\n",
            "hirabayashi\t(flat)\t-->\tgordon\n",
            "coram\t(flat)\t-->\tgordon\n",
            "nobis\t(flat)\t-->\tcoram\n",
            "case\t(compound)\t-->\tfebruary\n",
            "february\t(obj)\t-->\tinterviewed\n",
            "among\t(case)\t-->\teffort\n",
            "critical\t(amod)\t-->\teffort\n",
            "work\t(compound)\t-->\tdefeated\n",
            "successfully\t(advmod)\t-->\tdefeated\n",
            "defeated\t(amod)\t-->\teffort\n",
            "government\t(compound)\t-->\teffort\n",
            "effort\t(nmod)\t-->\tfebruary\n",
            "dismiss\t(acl)\t-->\tfebruary\n",
            "hirabayashi\t(compound)\t-->\tcase\n",
            "case\t(compound)\t-->\tled\n",
            "led\t(amod)\t-->\tconviction\n",
            "overturning\t(amod)\t-->\tconviction\n",
            "hirabayashi\t(compound)\t-->\tconviction\n",
            "conviction\t(obj)\t-->\tresisting\n",
            "resisting\t(acl)\t-->\telaine\n",
            "curfew\t(compound)\t-->\texclusion\n",
            "exclusion\t(compound)\t-->\torder\n",
            "order\t(obj)\t-->\tresisting\n",
            "promulgated\t(acl)\t-->\torder\n",
            "bannai\t(compound)\t-->\tpresident\n",
            "seattle\t(compound)\t-->\tpresident\n",
            "jacl\t(compound)\t-->\tpresident\n",
            "president\t(compound)\t-->\tchapter\n",
            "pivotal\t(amod)\t-->\tperiod\n",
            "period\t(compound)\t-->\tchapter\n",
            "chapter\t(compound)\t-->\tperiod\n",
            "period\t(compound)\t-->\tchapter\n",
            "chapter\t(appos)\t-->\torder\n",
            "expanded\t(amod)\t-->\twork\n",
            "work\t(appos)\t-->\tchapter\n",
            "toward\t(case)\t-->\tredress\n",
            "wwii\t(compound)\t-->\tredress\n",
            "japanese\t(amod)\t-->\tredress\n",
            "american\t(amod)\t-->\tredress\n",
            "redress\t(compound)\t-->\tchapter\n",
            "aging\t(compound)\t-->\tissue\n",
            "health\t(compound)\t-->\tissue\n",
            "issue\t(compound)\t-->\tbannai\n",
            "issei\t(compound)\t-->\tbannai\n",
            "nisei\t(compound)\t-->\tbannai\n",
            "youth\t(compound)\t-->\tprogramming\n",
            "programming\t(compound)\t-->\tbannai\n",
            "bannai\t(nsubj)\t-->\tforged\n",
            "also\t(advmod)\t-->\tforged\n",
            "forged\t(parataxis)\t-->\tinterviewed\n",
            "partnership\t(compound)\t-->\tinjustice\n",
            "japanese\t(amod)\t-->\tinjustice\n",
            "canadian\t(amod)\t-->\tshare\n",
            "share\t(compound)\t-->\tinjustice\n",
            "wwii\t(compound)\t-->\texperience\n",
            "experience\t(compound)\t-->\tinjustice\n",
            "injustice\t(obj)\t-->\tforged\n",
            "bannai\t(compound)\t-->\twoman\n",
            "third\t(amod)\t-->\twoman\n",
            "woman\t(compound)\t-->\tchapter\n",
            "chapter\t(appos)\t-->\tinjustice\n",
            "president\t(appos)\t-->\tchapter\n",
            "one\t(nummod)\t-->\tpresident\n",
            "earliest\t(amod)\t-->\tpresident\n",
            "sansei\t(compound)\t-->\tchapter\n",
            "chapter\t(compound)\t-->\tpresident\n",
            "president\t(appos)\t-->\tpresident\n",
            "bannai\t(compound)\t-->\tboard\n",
            "board\t(compound)\t-->\tpresident\n",
            "first\t(amod)\t-->\tsansei\n",
            "seattle\t(compound)\t-->\tboard\n",
            "jacl\t(compound)\t-->\tboard\n",
            "board\t(compound)\t-->\tsansei\n",
            "majority\t(compound)\t-->\tsansei\n",
            "sansei\t(appos)\t-->\tpresident\n",
            "also\t(advmod)\t-->\tmembership\n",
            "majority\t(amod)\t-->\tmembership\n",
            "female\t(amod)\t-->\tmembership\n",
            "membership\t(appos)\t-->\tpresident\n",
            "\n",
            "--- Sentence ---\n",
            "sansei female born los angeles california grew gardena california surrounded large japanese american community influenced father role community politics mother emphasis education attended university california santa barbara became increasingly aware japanese american history issue ethnic identity racial inequality attended university san francisco school law honed commitment political social activism year law school joined team lawyer working reopen supreme court decision korematsu united state convicted violating exclusion order world war ii mr korematsu case went way supreme court exclusion incarceration japanese american upheld constitutional based government argument military necessity petition writ error coram nobis establishing case premised error fact withheld judge defense prosecution legal team reopened case provided evidence factual underpinnings exclusion order fraudulent successfully korematsu conviction vacated well handful similar conviction interview m bannai discusses coram nobis legal team support effort among japanese american community personal lesson gained part effort\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "sansei\t(compound)\t-->\tborn\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(reparandum)\t-->\tbecame\n",
            "gardena\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tgrew\n",
            "surrounded\t(amod)\t-->\tcalifornia\n",
            "large\t(amod)\t-->\tmother\n",
            "japanese\t(amod)\t-->\tmother\n",
            "american\t(amod)\t-->\tcommunity\n",
            "community\t(compound)\t-->\tinfluenced\n",
            "influenced\t(amod)\t-->\tfather\n",
            "father\t(compound)\t-->\tmother\n",
            "role\t(compound)\t-->\tmother\n",
            "community\t(compound)\t-->\tmother\n",
            "politics\t(compound)\t-->\tmother\n",
            "mother\t(compound)\t-->\teducation\n",
            "emphasis\t(compound)\t-->\teducation\n",
            "education\t(compound)\t-->\tbarbara\n",
            "attended\t(amod)\t-->\tbarbara\n",
            "university\t(compound)\t-->\tbarbara\n",
            "california\t(compound)\t-->\tbarbara\n",
            "santa\t(compound)\t-->\tbarbara\n",
            "barbara\t(nsubj)\t-->\tbecame\n",
            "became\t(root)\t-->\tROOT\n",
            "increasingly\t(advmod)\t-->\taware\n",
            "aware\t(xcomp)\t-->\tbecame\n",
            "japanese\t(advmod)\t-->\taware\n",
            "american\t(amod)\t-->\tissue\n",
            "history\t(compound)\t-->\tissue\n",
            "issue\t(compound)\t-->\tinequality\n",
            "ethnic\t(amod)\t-->\tidentity\n",
            "identity\t(compound)\t-->\tinequality\n",
            "racial\t(amod)\t-->\tinequality\n",
            "inequality\t(obl)\t-->\taware\n",
            "attended\t(amod)\t-->\tschool\n",
            "university\t(compound)\t-->\tcommitment\n",
            "san\t(compound)\t-->\tschool\n",
            "francisco\t(flat)\t-->\tsan\n",
            "school\t(compound)\t-->\tcommitment\n",
            "law\t(compound)\t-->\thoned\n",
            "honed\t(amod)\t-->\tschool\n",
            "commitment\t(compound)\t-->\tschool\n",
            "political\t(amod)\t-->\tschool\n",
            "social\t(amod)\t-->\tyear\n",
            "activism\t(compound)\t-->\tyear\n",
            "year\t(compound)\t-->\tschool\n",
            "law\t(compound)\t-->\tschool\n",
            "school\t(parataxis)\t-->\taware\n",
            "joined\t(acl)\t-->\tschool\n",
            "team\t(compound)\t-->\tlawyer\n",
            "lawyer\t(obj)\t-->\tjoined\n",
            "working\t(acl)\t-->\tschool\n",
            "reopen\t(obj)\t-->\tworking\n",
            "supreme\t(amod)\t-->\tcourt\n",
            "court\t(compound)\t-->\tdecision\n",
            "decision\t(compound)\t-->\tkorematsu\n",
            "korematsu\t(compound)\t-->\tstate\n",
            "united\t(amod)\t-->\tstate\n",
            "state\t(compound)\t-->\tconvicted\n",
            "convicted\t(amod)\t-->\tcase\n",
            "violating\t(amod)\t-->\tcase\n",
            "exclusion\t(compound)\t-->\torder\n",
            "order\t(compound)\t-->\twar\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tcase\n",
            "ii\t(nummod)\t-->\twar\n",
            "mr\t(compound)\t-->\tcase\n",
            "korematsu\t(flat)\t-->\tmr\n",
            "case\t(nsubj)\t-->\twent\n",
            "went\t(parataxis)\t-->\tgrew\n",
            "way\t(obj)\t-->\twent\n",
            "supreme\t(amod)\t-->\tcourt\n",
            "court\t(compound)\t-->\texclusion\n",
            "exclusion\t(compound)\t-->\tincarceration\n",
            "incarceration\t(obj)\t-->\twent\n",
            "japanese\t(amod)\t-->\targument\n",
            "american\t(amod)\t-->\targument\n",
            "upheld\t(amod)\t-->\targument\n",
            "constitutional\t(compound)\t-->\tbased\n",
            "based\t(amod)\t-->\tnobis\n",
            "government\t(compound)\t-->\targument\n",
            "argument\t(compound)\t-->\tnobis\n",
            "military\t(amod)\t-->\tnecessity\n",
            "necessity\t(compound)\t-->\tnobis\n",
            "petition\t(compound)\t-->\tnobis\n",
            "writ\t(compound)\t-->\tnobis\n",
            "error\t(compound)\t-->\tnobis\n",
            "coram\t(compound)\t-->\tnobis\n",
            "nobis\t(appos)\t-->\tincarceration\n",
            "establishing\t(acl)\t-->\tnobis\n",
            "case\t(compound)\t-->\tpremised\n",
            "premised\t(amod)\t-->\tfact\n",
            "error\t(compound)\t-->\tfact\n",
            "fact\t(obj)\t-->\testablishing\n",
            "withheld\t(amod)\t-->\tteam\n",
            "judge\t(compound)\t-->\tprosecution\n",
            "defense\t(compound)\t-->\tprosecution\n",
            "prosecution\t(compound)\t-->\tteam\n",
            "legal\t(amod)\t-->\tteam\n",
            "team\t(nsubj)\t-->\treopened\n",
            "reopened\t(parataxis)\t-->\tgrew\n",
            "case\t(obj)\t-->\treopened\n",
            "provided\t(amod)\t-->\tevidence\n",
            "evidence\t(obj)\t-->\treopened\n",
            "factual\t(amod)\t-->\torder\n",
            "underpinnings\t(compound)\t-->\torder\n",
            "exclusion\t(compound)\t-->\torder\n",
            "order\t(compound)\t-->\tconviction\n",
            "fraudulent\t(amod)\t-->\tconviction\n",
            "successfully\t(advmod)\t-->\tkorematsu\n",
            "korematsu\t(compound)\t-->\tconviction\n",
            "conviction\t(nsubj)\t-->\tvacated\n",
            "vacated\t(parataxis)\t-->\treopened\n",
            "well\t(advmod)\t-->\thandful\n",
            "handful\t(amod)\t-->\tm\n",
            "similar\t(amod)\t-->\tm\n",
            "conviction\t(compound)\t-->\tinterview\n",
            "interview\t(compound)\t-->\tm\n",
            "m\t(nsubj)\t-->\tdiscusses\n",
            "bannai\t(flat)\t-->\tm\n",
            "discusses\t(parataxis)\t-->\tvacated\n",
            "coram\t(compound)\t-->\tnobis\n",
            "nobis\t(compound)\t-->\teffort\n",
            "legal\t(amod)\t-->\tteam\n",
            "team\t(compound)\t-->\tsupport\n",
            "support\t(compound)\t-->\teffort\n",
            "effort\t(obj)\t-->\tdiscusses\n",
            "among\t(case)\t-->\tlesson\n",
            "japanese\t(amod)\t-->\tlesson\n",
            "american\t(amod)\t-->\tlesson\n",
            "community\t(compound)\t-->\tlesson\n",
            "personal\t(amod)\t-->\tlesson\n",
            "lesson\t(nmod)\t-->\teffort\n",
            "gained\t(parataxis)\t-->\tdiscusses\n",
            "part\t(compound)\t-->\teffort\n",
            "effort\t(obj)\t-->\tgained\n",
            "\n",
            "--- Sentence ---\n",
            "nisei male born july delta colorado grew small mining farming town colorado utah arizona family moved boyle height los angeles california area graduating high school tested discrimination employment practice eventually succeeded obtaining job bank world war ii family held manzanar concentration camp california mr bannai joined nd regimental combat team later transferred military intelligence service served new guinea elsewhere overseas interpreter allied translator interpreter service atis interpreted surrender japanese force ceremony indonesia married eventually resettled gardena california worked floral industry founding bannai realty insurance company extremely active community civic volunteer mr bannai joined elk club well many veteran organization elected gardena city council elected california state legislature mr bannai became executive director commission wartime relocation internment civilian cwric appointed chief director memorial affair department veteran administration president ronald reagan\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tarea\n",
            "male\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tjuly\n",
            "july\t(compound)\t-->\tdelta\n",
            "delta\t(compound)\t-->\tcolorado\n",
            "colorado\t(nsubj)\t-->\tgrew\n",
            "grew\t(compound)\t-->\tarea\n",
            "small\t(amod)\t-->\tarea\n",
            "mining\t(compound)\t-->\tfarming\n",
            "farming\t(compound)\t-->\ttown\n",
            "town\t(compound)\t-->\tarea\n",
            "colorado\t(compound)\t-->\tarea\n",
            "utah\t(compound)\t-->\tarea\n",
            "arizona\t(compound)\t-->\tarea\n",
            "family\t(compound)\t-->\theight\n",
            "moved\t(amod)\t-->\tarea\n",
            "boyle\t(compound)\t-->\theight\n",
            "height\t(compound)\t-->\tarea\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tarea\n",
            "california\t(compound)\t-->\tarea\n",
            "area\t(nsubj)\t-->\tsucceeded\n",
            "graduating\t(acl)\t-->\tarea\n",
            "high\t(amod)\t-->\tschool\n",
            "school\t(compound)\t-->\ttested\n",
            "tested\t(amod)\t-->\tpractice\n",
            "discrimination\t(compound)\t-->\tpractice\n",
            "employment\t(compound)\t-->\tpractice\n",
            "practice\t(obj)\t-->\tgraduating\n",
            "eventually\t(advmod)\t-->\tsucceeded\n",
            "succeeded\t(root)\t-->\tROOT\n",
            "obtaining\t(xcomp)\t-->\tsucceeded\n",
            "job\t(compound)\t-->\tbank\n",
            "bank\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\tobtaining\n",
            "ii\t(compound)\t-->\tcalifornia\n",
            "family\t(compound)\t-->\theld\n",
            "held\t(amod)\t-->\tcalifornia\n",
            "manzanar\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tjoined\n",
            "mr\t(appos)\t-->\tcalifornia\n",
            "bannai\t(nsubj)\t-->\tjoined\n",
            "joined\t(parataxis)\t-->\tsucceeded\n",
            "nd\t(compound)\t-->\tteam\n",
            "regimental\t(amod)\t-->\tteam\n",
            "combat\t(compound)\t-->\tteam\n",
            "team\t(obj)\t-->\tjoined\n",
            "later\t(advmod)\t-->\ttransferred\n",
            "transferred\t(conj)\t-->\tjoined\n",
            "military\t(amod)\t-->\tservice\n",
            "intelligence\t(compound)\t-->\tservice\n",
            "service\t(obj)\t-->\ttransferred\n",
            "served\t(acl)\t-->\tservice\n",
            "new\t(amod)\t-->\tguinea\n",
            "guinea\t(obj)\t-->\tserved\n",
            "elsewhere\t(advmod)\t-->\tserved\n",
            "overseas\t(amod)\t-->\tatis\n",
            "interpreter\t(compound)\t-->\tatis\n",
            "allied\t(amod)\t-->\tatis\n",
            "translator\t(compound)\t-->\tatis\n",
            "interpreter\t(compound)\t-->\tatis\n",
            "service\t(compound)\t-->\tatis\n",
            "atis\t(nsubj)\t-->\tinterpreted\n",
            "interpreted\t(parataxis)\t-->\tgrew\n",
            "surrender\t(obj)\t-->\tinterpreted\n",
            "japanese\t(amod)\t-->\tforce\n",
            "force\t(compound)\t-->\tceremony\n",
            "ceremony\t(compound)\t-->\tindonesia\n",
            "indonesia\t(obj)\t-->\tinterpreted\n",
            "married\t(acl)\t-->\tindonesia\n",
            "eventually\t(advmod)\t-->\tresettled\n",
            "resettled\t(xcomp)\t-->\tworked\n",
            "gardena\t(compound)\t-->\tcalifornia\n",
            "california\t(obj)\t-->\tworked\n",
            "worked\t(amod)\t-->\tindonesia\n",
            "floral\t(amod)\t-->\tindustry\n",
            "industry\t(obj)\t-->\tfounding\n",
            "founding\t(amod)\t-->\tvolunteer\n",
            "bannai\t(compound)\t-->\tcompany\n",
            "realty\t(compound)\t-->\tcompany\n",
            "insurance\t(compound)\t-->\tcompany\n",
            "company\t(compound)\t-->\tvolunteer\n",
            "extremely\t(advmod)\t-->\tactive\n",
            "active\t(amod)\t-->\tvolunteer\n",
            "community\t(compound)\t-->\tvolunteer\n",
            "civic\t(amod)\t-->\tvolunteer\n",
            "volunteer\t(obj)\t-->\tworked\n",
            "mr\t(appos)\t-->\tvolunteer\n",
            "bannai\t(flat)\t-->\tmr\n",
            "joined\t(acl:relcl)\t-->\tvolunteer\n",
            "elk\t(compound)\t-->\tclub\n",
            "club\t(obj)\t-->\tjoined\n",
            "well\t(advmod)\t-->\tmany\n",
            "many\t(amod)\t-->\tlegislature\n",
            "veteran\t(amod)\t-->\torganization\n",
            "organization\t(compound)\t-->\tlegislature\n",
            "elected\t(amod)\t-->\tlegislature\n",
            "gardena\t(compound)\t-->\tcouncil\n",
            "city\t(compound)\t-->\tcouncil\n",
            "council\t(compound)\t-->\telected\n",
            "elected\t(amod)\t-->\tlegislature\n",
            "california\t(compound)\t-->\tstate\n",
            "state\t(compound)\t-->\tlegislature\n",
            "legislature\t(nsubj)\t-->\tbecame\n",
            "mr\t(flat)\t-->\tlegislature\n",
            "bannai\t(flat)\t-->\tlegislature\n",
            "became\t(parataxis)\t-->\tjoined\n",
            "executive\t(amod)\t-->\tcommission\n",
            "director\t(compound)\t-->\tcommission\n",
            "commission\t(compound)\t-->\trelocation\n",
            "wartime\t(amod)\t-->\tcwric\n",
            "relocation\t(compound)\t-->\tcwric\n",
            "internment\t(amod)\t-->\tcwric\n",
            "civilian\t(amod)\t-->\tcwric\n",
            "cwric\t(xcomp)\t-->\tbecame\n",
            "appointed\t(acl)\t-->\tcwric\n",
            "chief\t(amod)\t-->\taffair\n",
            "director\t(compound)\t-->\tdepartment\n",
            "memorial\t(compound)\t-->\taffair\n",
            "affair\t(compound)\t-->\tdepartment\n",
            "department\t(xcomp)\t-->\tappointed\n",
            "veteran\t(compound)\t-->\tpresident\n",
            "administration\t(flat)\t-->\tveteran\n",
            "president\t(appos)\t-->\tdepartment\n",
            "ronald\t(appos)\t-->\tpresident\n",
            "reagan\t(appos)\t-->\tpresident\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born july bedderavia california given adoption parent couple could child grew family farm one oldest nisei santa barbara area california incarcerated puyallup assembly center washington minidoka concentration camp idaho widow clarence arai lawyer key figure founding japanese american citizen league interview discusses childhood memory married clarence turbulent war year war supported family cared ailing clarence death remarried george bartholomew\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tcouple\n",
            "female\t(amod)\t-->\tcouple\n",
            "born\t(amod)\t-->\tcouple\n",
            "july\t(compound)\t-->\tcouple\n",
            "bedderavia\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tcouple\n",
            "given\t(amod)\t-->\tcouple\n",
            "adoption\t(compound)\t-->\tparent\n",
            "parent\t(compound)\t-->\tcouple\n",
            "couple\t(nsubj)\t-->\tgrew\n",
            "could\t(aux)\t-->\tgrew\n",
            "child\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "family\t(compound)\t-->\tfarm\n",
            "farm\t(obj)\t-->\tgrew\n",
            "one\t(nummod)\t-->\tinterview\n",
            "oldest\t(amod)\t-->\tinterview\n",
            "nisei\t(compound)\t-->\tarea\n",
            "santa\t(compound)\t-->\tarea\n",
            "barbara\t(compound)\t-->\tarea\n",
            "area\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tcenter\n",
            "incarcerated\t(amod)\t-->\tcenter\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tlawyer\n",
            "washington\t(compound)\t-->\tcamp\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\twidow\n",
            "idaho\t(compound)\t-->\tlawyer\n",
            "widow\t(compound)\t-->\tlawyer\n",
            "clarence\t(compound)\t-->\tlawyer\n",
            "arai\t(compound)\t-->\tlawyer\n",
            "lawyer\t(compound)\t-->\tinterview\n",
            "key\t(amod)\t-->\tfigure\n",
            "figure\t(compound)\t-->\tinterview\n",
            "founding\t(amod)\t-->\tinterview\n",
            "japanese\t(amod)\t-->\tinterview\n",
            "american\t(amod)\t-->\tinterview\n",
            "citizen\t(compound)\t-->\tleague\n",
            "league\t(compound)\t-->\tinterview\n",
            "interview\t(obj)\t-->\tdiscusses\n",
            "discusses\t(acl:relcl)\t-->\tfarm\n",
            "childhood\t(compound)\t-->\tmemory\n",
            "memory\t(compound)\t-->\tmarried\n",
            "married\t(amod)\t-->\twar\n",
            "clarence\t(compound)\t-->\twar\n",
            "turbulent\t(amod)\t-->\twar\n",
            "war\t(compound)\t-->\tyear\n",
            "year\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\tdiscusses\n",
            "supported\t(amod)\t-->\tdeath\n",
            "family\t(compound)\t-->\tcared\n",
            "cared\t(amod)\t-->\tdeath\n",
            "ailing\t(amod)\t-->\tdeath\n",
            "clarence\t(compound)\t-->\tdeath\n",
            "death\t(nsubj)\t-->\tremarried\n",
            "remarried\t(parataxis)\t-->\tdiscusses\n",
            "george\t(compound)\t-->\tbartholomew\n",
            "bartholomew\t(obj)\t-->\tremarried\n",
            "\n",
            "--- Sentence ---\n",
            "sansei male family owned linc tackle longtime japanese american business seattle washington prior world war ii father became owner togo tackle shop war removed puyallup assembly center washington minidoka concentration camp idaho family camp prewar insurance agent paid premium could retain insurance policy returned seattle gerald father able use fund policy establish linc\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "sansei\t(compound)\t-->\tfather\n",
            "male\t(amod)\t-->\tfather\n",
            "family\t(obl:npmod)\t-->\towned\n",
            "owned\t(amod)\t-->\ttackle\n",
            "linc\t(compound)\t-->\ttackle\n",
            "tackle\t(compound)\t-->\tfather\n",
            "longtime\t(amod)\t-->\tfather\n",
            "japanese\t(amod)\t-->\tfather\n",
            "american\t(amod)\t-->\tfather\n",
            "business\t(compound)\t-->\tfather\n",
            "seattle\t(compound)\t-->\tfather\n",
            "washington\t(compound)\t-->\tfather\n",
            "prior\t(amod)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(compound)\t-->\tfather\n",
            "ii\t(compound)\t-->\tfather\n",
            "father\t(nsubj)\t-->\tbecame\n",
            "became\t(root)\t-->\tROOT\n",
            "owner\t(compound)\t-->\twar\n",
            "togo\t(compound)\t-->\ttackle\n",
            "tackle\t(compound)\t-->\tshop\n",
            "shop\t(compound)\t-->\twar\n",
            "war\t(xcomp)\t-->\tbecame\n",
            "removed\t(acl)\t-->\twar\n",
            "puyallup\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tagent\n",
            "washington\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcamp\n",
            "idaho\t(compound)\t-->\tagent\n",
            "family\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tagent\n",
            "prewar\t(amod)\t-->\tagent\n",
            "insurance\t(compound)\t-->\tagent\n",
            "agent\t(nsubj)\t-->\tretain\n",
            "paid\t(amod)\t-->\tpremium\n",
            "premium\t(nmod)\t-->\tagent\n",
            "could\t(aux)\t-->\tretain\n",
            "retain\t(acl:relcl)\t-->\twar\n",
            "insurance\t(compound)\t-->\tpolicy\n",
            "policy\t(obj)\t-->\tretain\n",
            "returned\t(xcomp)\t-->\tretain\n",
            "seattle\t(compound)\t-->\tpolicy\n",
            "gerald\t(compound)\t-->\tfather\n",
            "father\t(compound)\t-->\tpolicy\n",
            "able\t(amod)\t-->\tpolicy\n",
            "use\t(compound)\t-->\tfund\n",
            "fund\t(compound)\t-->\tpolicy\n",
            "policy\t(obj)\t-->\tretain\n",
            "establish\t(xcomp)\t-->\tretain\n",
            "linc\t(obj)\t-->\testablish\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born october los angeles california grew los angeles involved early age traditional japanese dancing stage performance world war ii removed family santa anita assembly center california rohwer concentration camp arkansas leaving camp family resettled denver colorado ten year returning california war pursued career model actor time got involved numerous japanese american community group\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tcalifornia\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "october\t(compound)\t-->\tcalifornia\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "los\t(compound)\t-->\tangeles\n",
            "angeles\t(obj)\t-->\tgrew\n",
            "involved\t(acl)\t-->\tangeles\n",
            "early\t(amod)\t-->\tage\n",
            "age\t(obj)\t-->\tinvolved\n",
            "traditional\t(amod)\t-->\twar\n",
            "japanese\t(amod)\t-->\twar\n",
            "dancing\t(compound)\t-->\tstage\n",
            "stage\t(compound)\t-->\twar\n",
            "performance\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(parataxis)\t-->\tgrew\n",
            "family\t(compound)\t-->\tsanta\n",
            "santa\t(compound)\t-->\tanita\n",
            "anita\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tcalifornia\n",
            "california\t(compound)\t-->\tcamp\n",
            "rohwer\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tarkansas\n",
            "arkansas\t(compound)\t-->\tcamp\n",
            "leaving\t(amod)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcolorado\n",
            "family\t(compound)\t-->\tcolorado\n",
            "resettled\t(amod)\t-->\tcolorado\n",
            "denver\t(compound)\t-->\tcolorado\n",
            "colorado\t(compound)\t-->\twar\n",
            "ten\t(nummod)\t-->\tyear\n",
            "year\t(obl:npmod)\t-->\treturning\n",
            "returning\t(amod)\t-->\twar\n",
            "california\t(compound)\t-->\twar\n",
            "war\t(obj)\t-->\tpursued\n",
            "pursued\t(acl)\t-->\ttime\n",
            "career\t(compound)\t-->\tmodel\n",
            "model\t(compound)\t-->\ttime\n",
            "actor\t(compound)\t-->\ttime\n",
            "time\t(nsubj:pass)\t-->\tinvolved\n",
            "got\t(aux:pass)\t-->\tinvolved\n",
            "involved\t(parataxis)\t-->\tgrew\n",
            "numerous\t(amod)\t-->\tgroup\n",
            "japanese\t(amod)\t-->\tgroup\n",
            "american\t(amod)\t-->\tgroup\n",
            "community\t(compound)\t-->\tgroup\n",
            "group\t(obj)\t-->\tinvolved\n",
            "\n",
            "--- Sentence ---\n",
            "nisei female born july walnut grove california grew walnut grove bombing pearl harbor removed family merced assembly center california granada amache concentration camp colorado left camp attend boarding school new york eventually returning walnut grove marion passed away december\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tharbor\n",
            "female\t(amod)\t-->\tborn\n",
            "born\t(amod)\t-->\tcalifornia\n",
            "july\t(amod)\t-->\tcalifornia\n",
            "walnut\t(compound)\t-->\tgrove\n",
            "grove\t(compound)\t-->\tcalifornia\n",
            "california\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "walnut\t(compound)\t-->\tgrove\n",
            "grove\t(compound)\t-->\tbombing\n",
            "bombing\t(compound)\t-->\tharbor\n",
            "pearl\t(compound)\t-->\tharbor\n",
            "harbor\t(obj)\t-->\tgrew\n",
            "removed\t(acl)\t-->\tharbor\n",
            "family\t(compound)\t-->\tcenter\n",
            "merced\t(amod)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\tgranada\n",
            "california\t(compound)\t-->\tgranada\n",
            "granada\t(compound)\t-->\tcolorado\n",
            "amache\t(compound)\t-->\tcamp\n",
            "concentration\t(compound)\t-->\tcamp\n",
            "camp\t(compound)\t-->\tcolorado\n",
            "colorado\t(nsubj)\t-->\tleft\n",
            "left\t(parataxis)\t-->\tgrew\n",
            "camp\t(obj)\t-->\tleft\n",
            "attend\t(xcomp)\t-->\tleft\n",
            "boarding\t(compound)\t-->\tschool\n",
            "school\t(obj)\t-->\tattend\n",
            "new\t(amod)\t-->\tyork\n",
            "york\t(nsubj)\t-->\treturning\n",
            "eventually\t(advmod)\t-->\treturning\n",
            "returning\t(advcl)\t-->\tattend\n",
            "walnut\t(compound)\t-->\tgrove\n",
            "grove\t(compound)\t-->\tmarion\n",
            "marion\t(nsubj)\t-->\tpassed\n",
            "passed\t(parataxis)\t-->\tleft\n",
            "away\t(advmod)\t-->\tpassed\n",
            "december\t(obj)\t-->\tpassed\n",
            "\n",
            "--- Sentence ---\n",
            "female filipino american descent born seattle washington grew bainbridge island washington granddaughter felix narte one well known filipino men bainbridge island worked japanese american strawberry farmer\n",
            "\n",
            "Constituency Tree:\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "female\t(amod)\t-->\twashington\n",
            "filipino\t(amod)\t-->\tdescent\n",
            "american\t(amod)\t-->\tdescent\n",
            "descent\t(compound)\t-->\tborn\n",
            "born\t(amod)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(nsubj)\t-->\tgrew\n",
            "grew\t(root)\t-->\tROOT\n",
            "bainbridge\t(compound)\t-->\tisland\n",
            "island\t(obj)\t-->\tgrew\n",
            "washington\t(compound)\t-->\tgranddaughter\n",
            "granddaughter\t(appos)\t-->\tisland\n",
            "felix\t(appos)\t-->\tgranddaughter\n",
            "narte\t(flat)\t-->\tfelix\n",
            "one\t(nummod)\t-->\tisland\n",
            "well\t(advmod)\t-->\tknown\n",
            "known\t(amod)\t-->\tisland\n",
            "filipino\t(amod)\t-->\tisland\n",
            "men\t(compound)\t-->\tisland\n",
            "bainbridge\t(compound)\t-->\tisland\n",
            "island\t(nsubj)\t-->\tworked\n",
            "worked\t(parataxis)\t-->\tgrew\n",
            "japanese\t(amod)\t-->\tfarmer\n",
            "american\t(amod)\t-->\tfarmer\n",
            "strawberry\t(compound)\t-->\tfarmer\n",
            "farmer\t(obj)\t-->\tworked\n",
            "\n",
            "=== (1) POS Totals ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   NOUN_total  VERB_total  ADJ_total  ADV_total\n",
              "0       31102        8719       5175        750"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e59198a2-ba0e-4ff0-b9f7-7b660245f7b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NOUN_total</th>\n",
              "      <th>VERB_total</th>\n",
              "      <th>ADJ_total</th>\n",
              "      <th>ADV_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31102</td>\n",
              "      <td>8719</td>\n",
              "      <td>5175</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e59198a2-ba0e-4ff0-b9f7-7b660245f7b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e59198a2-ba0e-4ff0-b9f7-7b660245f7b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e59198a2-ba0e-4ff0-b9f7-7b660245f7b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\\\n[Note] No sentence found for the example explanation\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"NOUN_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 31102,\n        \"max\": 31102,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          31102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VERB_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 8719,\n        \"max\": 8719,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8719\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ADJ_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 5175,\n        \"max\": 5175,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5175\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ADV_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 750,\n        \"max\": 750,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          750\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== (3) NER Counts (PERSON/ORG/GPE/LOC/PRODUCT/DATE) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    label  count\n",
              "0    DATE    523\n",
              "1     GPE   1363\n",
              "2     LOC     10\n",
              "3     ORG      7\n",
              "4  PERSON    137"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42cf8b02-7286-4994-a9b5-4634bfce594b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DATE</td>\n",
              "      <td>523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GPE</td>\n",
              "      <td>1363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOC</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ORG</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PERSON</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42cf8b02-7286-4994-a9b5-4634bfce594b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42cf8b02-7286-4994-a9b5-4634bfce594b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42cf8b02-7286-4994-a9b5-4634bfce594b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36b906a7-ee87-4f88-a623-1fff5e0bba24\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36b906a7-ee87-4f88-a623-1fff5e0bba24')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36b906a7-ee87-4f88-a623-1fff5e0bba24 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_37c26030-0362-4ac1-9ff1-a1df1c0f0938\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ner_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_37c26030-0362-4ac1-9ff1-a1df1c0f0938 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ner_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ner_df",
              "summary": "{\n  \"name\": \"ner_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"GPE\",\n          \"PERSON\",\n          \"LOC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 573,\n        \"min\": 7,\n        \"max\": 1363,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1363,\n          137,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved constituency trees to: analysis_outputs_stanza/constituency_trees.txt\n",
            "Saved dependency trees to:   analysis_outputs_stanza/dependency_trees.txt\n",
            "Saved POS totals to:         analysis_outputs_stanza/pos_totals.csv\n",
            "Saved NER counts to:         analysis_outputs_stanza/ner_counts.csv\n",
            "\n",
            "=== (2) Example Sentence & Explanation ===\n",
            "Sentence:\n",
            "nisei female born may selleck washington spent much childhood beaverton oregon father owned farm influenced early age parent conversion christianity world war ii removed portland assembly center oregon minidoka concentration camp idaho war worked establish successful volunteer program feed homeless seattle washington\n",
            "\n",
            "Constituency Tree (bracketed):\n",
            "(NO_PARSE)\n",
            "\n",
            "Dependency Tree (token (dep) --> head):\n",
            "nisei\t(amod)\t-->\tfemale\n",
            "female\t(nsubj)\t-->\tspent\n",
            "born\t(acl)\t-->\tfemale\n",
            "may\t(aux)\t-->\tselleck\n",
            "selleck\t(compound)\t-->\twashington\n",
            "washington\t(obj)\t-->\tborn\n",
            "spent\t(root)\t-->\tROOT\n",
            "much\t(amod)\t-->\tfarm\n",
            "childhood\t(compound)\t-->\tfather\n",
            "beaverton\t(compound)\t-->\tfather\n",
            "oregon\t(compound)\t-->\tfather\n",
            "father\t(compound)\t-->\tfarm\n",
            "owned\t(amod)\t-->\tfarm\n",
            "farm\t(obj)\t-->\tspent\n",
            "influenced\t(amod)\t-->\tfarm\n",
            "early\t(amod)\t-->\tage\n",
            "age\t(compound)\t-->\tconversion\n",
            "parent\t(compound)\t-->\tconversion\n",
            "conversion\t(compound)\t-->\twar\n",
            "christianity\t(compound)\t-->\tworld\n",
            "world\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tremoved\n",
            "ii\t(appos)\t-->\twar\n",
            "removed\t(parataxis)\t-->\tspent\n",
            "portland\t(compound)\t-->\tcenter\n",
            "assembly\t(compound)\t-->\tcenter\n",
            "center\t(compound)\t-->\twar\n",
            "oregon\t(compound)\t-->\tcenter\n",
            "minidoka\t(compound)\t-->\tconcentration\n",
            "concentration\t(compound)\t-->\twar\n",
            "camp\t(compound)\t-->\twar\n",
            "idaho\t(compound)\t-->\twar\n",
            "war\t(nsubj)\t-->\tworked\n",
            "worked\t(parataxis)\t-->\tremoved\n",
            "establish\t(xcomp)\t-->\tworked\n",
            "successful\t(amod)\t-->\twashington\n",
            "volunteer\t(compound)\t-->\tprogram\n",
            "program\t(compound)\t-->\tfeed\n",
            "feed\t(compound)\t-->\twashington\n",
            "homeless\t(amod)\t-->\twashington\n",
            "seattle\t(compound)\t-->\twashington\n",
            "washington\t(obj)\t-->\testablish\n",
            "\n",
            "Explanation:\n",
            "• Constituency parsing groups words into nested phrases (NP, VP, PP, etc.), showing the sentence as a hierarchy from S down to terminals.\n",
            "• Dependency parsing links each word to its syntactic head with labeled relations (e.g., nsubj, obj, obl). It emphasizes who governs whom rather than explicit phrase boundaries.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "# === Analysis with Stanza (POS, DEP, Constituency, NER) ===\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "import stanza\n",
        "\n",
        "# Build Stanza pipeline (CPU), now that runtime has restarted\n",
        "nlp = stanza.Pipeline(\n",
        "    'en',\n",
        "    processors='tokenize,pos,lemma,depparse,ner,constituency',\n",
        "    use_gpu=False,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# -------- Inputs / outputs --------\n",
        "IN_CSV = \"densho_narrators_clean.csv\"   # produced earlier\n",
        "TEXT_COL = \"clean_text\"\n",
        "\n",
        "OUT_DIR = \"analysis_outputs_stanza\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "CONSTIT_FILE = os.path.join(OUT_DIR, \"constituency_trees.txt\")\n",
        "DEPEND_FILE  = os.path.join(OUT_DIR, \"dependency_trees.txt\")\n",
        "POS_COUNTS_CSV = os.path.join(OUT_DIR, \"pos_totals.csv\")\n",
        "NER_COUNTS_CSV = os.path.join(OUT_DIR, \"ner_counts.csv\")\n",
        "\n",
        "PRINT_CAP_SENTENCES = 40     # set to None to print ALL sentences (very long)\n",
        "EXAMPLE_SENTENCE_INDEX = 0   # which sentence to explain\n",
        "\n",
        "# -------- Load data --------\n",
        "df = pd.read_csv(IN_CSV)\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found in {IN_CSV}. Run the cleaning step first.\")\n",
        "texts = [str(t).strip() for t in df[TEXT_COL].fillna(\"\") if str(t).strip()]\n",
        "\n",
        "# -------- Helpers --------\n",
        "def pos_bucket(upos):\n",
        "    if upos in (\"NOUN\", \"PROPN\"): return \"NOUN\"\n",
        "    if upos == \"VERB\": return \"VERB\"\n",
        "    if upos == \"ADJ\":  return \"ADJ\"\n",
        "    if upos == \"ADV\":  return \"ADV\"\n",
        "    return None\n",
        "\n",
        "def dependency_lines(sent):\n",
        "    # stanza Sentence.words: 1-based ids; head==0 means ROOT\n",
        "    id2txt = {w.id: w.text for w in sent.words}\n",
        "    lines = []\n",
        "    for w in sent.words:\n",
        "        head_text = \"ROOT\" if w.head == 0 else id2txt.get(w.head, \"ROOT\")\n",
        "        lines.append(f\"{w.text}\\t({w.deprel})\\t-->\\t{head_text}\")\n",
        "    return lines\n",
        "\n",
        "def constituency_string(sent):\n",
        "    try:\n",
        "        # sent.constituency is a tree; .to_string() returns bracketed string\n",
        "        return sent.constituency.to_string()\n",
        "    except Exception:\n",
        "        return \"(NO_PARSE)\"\n",
        "\n",
        "# -------- Aggregations --------\n",
        "pos_counter = Counter()\n",
        "ner_counter = Counter()\n",
        "ner_labels_of_interest = {\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"DATE\"}\n",
        "\n",
        "all_const_lines, all_dep_lines = [], []\n",
        "printed = 0\n",
        "global_sent_idx = 0\n",
        "\n",
        "example_sent_text = None\n",
        "example_const = None\n",
        "example_dep = None\n",
        "\n",
        "print(\"Processing documents... (POS/DEP/Constituency/NER via Stanza)\")\n",
        "for text in tqdm(texts, total=len(texts)):\n",
        "    if not text:\n",
        "        continue\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # POS & NER aggregates are per-doc\n",
        "    for sent in doc.sentences:\n",
        "        for w in sent.words:\n",
        "            b = pos_bucket(w.upos)\n",
        "            if b:\n",
        "                pos_counter[b] += 1\n",
        "\n",
        "    # NER: stanza exposes doc.ents (across sentences)\n",
        "    if getattr(doc, \"entities\", None):\n",
        "        for ent in doc.entities:\n",
        "            if ent.type in ner_labels_of_interest:\n",
        "                ner_counter[ent.type] += 1\n",
        "\n",
        "    # Trees per sentence\n",
        "    for sent in doc.sentences:\n",
        "        cstr = constituency_string(sent)\n",
        "        dlines = dependency_lines(sent)\n",
        "        all_const_lines.append(cstr)\n",
        "        all_dep_lines.extend(dlines)\n",
        "        all_dep_lines.append(\"\")  # spacer\n",
        "\n",
        "        if PRINT_CAP_SENTENCES is None or printed < PRINT_CAP_SENTENCES:\n",
        "            print(\"\\n--- Sentence ---\")\n",
        "            print(\" \".join([w.text for w in sent.words]))\n",
        "            print(\"\\nConstituency Tree:\")\n",
        "            print(cstr)\n",
        "            print(\"\\nDependency Tree (token (dep) --> head):\")\n",
        "            for ln in dlines:\n",
        "                print(ln)\n",
        "            printed += 1\n",
        "\n",
        "        if example_sent_text is None and global_sent_idx == EXAMPLE_SENTENCE_INDEX:\n",
        "            example_sent_text = \" \".join([w.text for w in sent.words])\n",
        "            example_const = cstr\n",
        "            example_dep = dlines\n",
        "        global_sent_idx += 1\n",
        "\n",
        "# -------- Save artifacts --------\n",
        "with open(CONSTIT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in all_const_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "with open(DEPEND_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in all_dep_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# (1) POS totals\n",
        "pos_totals = {\n",
        "    \"NOUN_total\": pos_counter.get(\"NOUN\", 0),\n",
        "    \"VERB_total\": pos_counter.get(\"VERB\", 0),\n",
        "    \"ADJ_total\":  pos_counter.get(\"ADJ\", 0),\n",
        "    \"ADV_total\":  pos_counter.get(\"ADV\", 0),\n",
        "}\n",
        "pd.DataFrame([pos_totals]).to_csv(POS_COUNTS_CSV, index=False)\n",
        "print(\"\\n=== (1) POS Totals ===\")\n",
        "display(pd.DataFrame([pos_totals]))\n",
        "\n",
        "# (3) NER counts (selected)\n",
        "ner_df = pd.DataFrame(\n",
        "    [{\"label\": k, \"count\": v} for k, v in sorted(ner_counter.items())]\n",
        ")\n",
        "ner_df.to_csv(NER_COUNTS_CSV, index=False)\n",
        "print(\"\\n=== (3) NER Counts (PERSON/ORG/GPE/LOC/PRODUCT/DATE) ===\")\n",
        "display(ner_df)\n",
        "\n",
        "print(f\"\\nSaved constituency trees to: {CONSTIT_FILE}\")\n",
        "print(f\"Saved dependency trees to:   {DEPEND_FILE}\")\n",
        "print(f\"Saved POS totals to:         {POS_COUNTS_CSV}\")\n",
        "print(f\"Saved NER counts to:         {NER_COUNTS_CSV}\")\n",
        "\n",
        "# (2) Explanation\n",
        "if example_sent_text:\n",
        "    print(\"\\n=== (2) Example Sentence & Explanation ===\")\n",
        "    print(\"Sentence:\")\n",
        "    print(example_sent_text)\n",
        "\n",
        "    print(\"\\nConstituency Tree (bracketed):\")\n",
        "    print(example_const)\n",
        "\n",
        "    print(\"\\nDependency Tree (token (dep) --> head):\")\n",
        "    for ln in example_dep:\n",
        "        print(ln)\n",
        "\n",
        "    print(\"\\nExplanation:\")\n",
        "    print(\n",
        "        \"• Constituency parsing groups words into nested phrases (NP, VP, PP, etc.), \"\n",
        "        \"showing the sentence as a hierarchy from S down to terminals.\\n\"\n",
        "        \"• Dependency parsing links each word to its syntactic head with labeled relations \"\n",
        "        \"(e.g., nsubj, obj, obl). It emphasizes who governs whom rather than explicit phrase boundaries.\"\n",
        "    )\n",
        "else:\n",
        "    print(\"\\n[Note] No sentence found for the example explanation.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART-1 (pagination fixed): Scrape GitHub Marketplace Actions → CSV\n",
        "#   pip install requests beautifulsoup4 pandas\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Dict, Tuple\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, FeatureNotFound\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"https://github.com\"\n",
        "# Use empty query to get the paginated listing (not just featured)\n",
        "BASE_LIST = \"https://github.com/marketplace?type=actions&query=\"\n",
        "OUTPUT_CSV = \"github_actions_marketplace_raw.csv\"\n",
        "\n",
        "# Controls\n",
        "MAX_PRODUCTS = 1000                # target cap\n",
        "TIME_LIMIT_SECONDS = 10 * 60       # stop after N seconds\n",
        "REQUEST_DELAY_RANGE = (0.6, 1.2)   # polite jitter\n",
        "TIMEOUT = 25\n",
        "MAX_PAGES = 500                    # hard stop in case of long pagination\n",
        "EMPTY_PAGE_TOLERANCE = 2           # stop after N consecutive empty pages\n",
        "\n",
        "def make_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "                       \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"),\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Referer\": BASE_LIST,\n",
        "        \"Connection\": \"keep-alive\",\n",
        "    })\n",
        "    retries = Retry(\n",
        "        total=5,\n",
        "        connect=5,\n",
        "        read=5,\n",
        "        backoff_factor=0.7,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=frozenset([\"GET\"]),\n",
        "        raise_on_status=False,\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=40)\n",
        "    s.mount(\"https://\", adapter)\n",
        "    s.mount(\"http://\", adapter)\n",
        "    return s\n",
        "\n",
        "def get_soup(session, url, timeout=TIMEOUT):\n",
        "    resp = session.get(url, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    html = resp.text\n",
        "    try:\n",
        "        return BeautifulSoup(html, \"lxml\")\n",
        "    except FeatureNotFound:\n",
        "        return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "def strict_action_cards(soup: BeautifulSoup) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Prefer strong matches: anchor href starts with /marketplace/actions/\n",
        "    Pull name from heading/anchor, description from nearby muted/paragraph text.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    seen = set()\n",
        "    containers = soup.select(\"article, li, div\")\n",
        "    for node in containers:\n",
        "        a = node.select_one('a[href^=\"/marketplace/actions/\"]')\n",
        "        if not a:\n",
        "            continue\n",
        "        href = a.get(\"href\", \"\").strip()\n",
        "        if not href or href in seen:\n",
        "            continue\n",
        "        seen.add(href)\n",
        "\n",
        "        # Product name\n",
        "        name = \"\"\n",
        "        for tag in (\"h3\", \"h2\", \"h4\"):\n",
        "            h = node.find(tag)\n",
        "            if h:\n",
        "                name = h.get_text(\" \", strip=True)\n",
        "                break\n",
        "        if not name:\n",
        "            name = a.get_text(\" \", strip=True)\n",
        "\n",
        "        # Description\n",
        "        desc = \"\"\n",
        "        cand = node.select_one(\"p, div.color-fg-muted, div.text-small, div.f6, span.color-fg-muted\")\n",
        "        if cand:\n",
        "            desc = cand.get_text(\" \", strip=True)\n",
        "        if not desc:\n",
        "            p = node.find(\"p\")\n",
        "            if p:\n",
        "                desc = p.get_text(\" \", strip=True)\n",
        "\n",
        "        results.append({\n",
        "            \"name\": name,\n",
        "            \"description\": desc,\n",
        "            \"url\": urljoin(BASE, href),\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def fallback_cards(soup: BeautifulSoup) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Looser heuristic: any marketplace link containing '/marketplace/actions/'\n",
        "    Useful in case markup differs across pages.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    seen = set()\n",
        "    for a in soup.select('a[href*=\"/marketplace/actions/\"]'):\n",
        "        href = a.get(\"href\", \"\").strip()\n",
        "        if not href or href in seen:\n",
        "            continue\n",
        "        seen.add(href)\n",
        "        name = a.get_text(\" \", strip=True)\n",
        "        if not name or len(name) < 2:\n",
        "            continue\n",
        "        # try close-by description\n",
        "        desc = \"\"\n",
        "        parent = a\n",
        "        for _ in range(4):\n",
        "            parent = parent.parent\n",
        "            if not parent:\n",
        "                break\n",
        "            if parent.name in (\"article\", \"li\", \"div\", \"section\"):\n",
        "                cand = parent.select_one(\"p, div.color-fg-muted, div.text-small, div.f6, span.color-fg-muted\")\n",
        "                if cand:\n",
        "                    desc = cand.get_text(\" \", strip=True)\n",
        "                    break\n",
        "        results.append({\n",
        "            \"name\": name,\n",
        "            \"description\": desc,\n",
        "            \"url\": urljoin(BASE, href),\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def build_page_url(page_num: int) -> str:\n",
        "    # Explicit page parameter\n",
        "    if page_num <= 1:\n",
        "        return BASE_LIST\n",
        "    return f\"{BASE_LIST}&page={page_num}\"\n",
        "\n",
        "def scrape_actions(max_products=MAX_PRODUCTS, time_limit_s=TIME_LIMIT_SECONDS) -> Tuple[pd.DataFrame, dict]:\n",
        "    session = make_session()\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    rows = []\n",
        "    seen_urls = set()\n",
        "    errors = 0\n",
        "    empty_streak = 0\n",
        "\n",
        "    page_counter = 0\n",
        "    for page in range(1, MAX_PAGES + 1):\n",
        "        if time.perf_counter() - t0 > time_limit_s:\n",
        "            print(f\"[INFO] Time limit reached. Stopping.\")\n",
        "            break\n",
        "        if len(rows) >= max_products:\n",
        "            break\n",
        "\n",
        "        url = build_page_url(page)\n",
        "        try:\n",
        "            soup = get_soup(session, url)\n",
        "        except Exception as e:\n",
        "            errors += 1\n",
        "            print(f\"[WARN] Failed {url}: {e}\")\n",
        "            # small backoff and continue; if many fail in a row, time limit will bail out\n",
        "            time.sleep(random.uniform(*REQUEST_DELAY_RANGE))\n",
        "            continue\n",
        "\n",
        "        page_counter += 1\n",
        "\n",
        "        # Try strict first; fall back if nothing found\n",
        "        cards = strict_action_cards(soup)\n",
        "        if not cards:\n",
        "            cards = fallback_cards(soup)\n",
        "\n",
        "        added = 0\n",
        "        for c in cards:\n",
        "            if c[\"url\"] in seen_urls:\n",
        "                continue\n",
        "            seen_urls.add(c[\"url\"])\n",
        "            rows.append({\n",
        "                \"product_name\": c[\"name\"],\n",
        "                \"description\": c[\"description\"],\n",
        "                \"url\": c[\"url\"],\n",
        "                \"page_number\": page,\n",
        "            })\n",
        "            added += 1\n",
        "            if len(rows) >= max_products:\n",
        "                break\n",
        "\n",
        "        print(f\"[INFO] Page {page}: found {len(cards)} action cards, added {added}, total {len(rows)}\")\n",
        "\n",
        "        # empty-page logic\n",
        "        if added == 0:\n",
        "            empty_streak += 1\n",
        "            if empty_streak >= EMPTY_PAGE_TOLERANCE:\n",
        "                print(f\"[INFO] {EMPTY_PAGE_TOLERANCE} consecutive empty pages. Stopping.\")\n",
        "                break\n",
        "        else:\n",
        "            empty_streak = 0\n",
        "\n",
        "        # polite pause\n",
        "        time.sleep(random.uniform(*REQUEST_DELAY_RANGE))\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"product_name\", \"description\", \"url\", \"page_number\"])\n",
        "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    report = {\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"pages_visited\": page_counter,\n",
        "        \"total_collected\": int(len(df)),\n",
        "        \"unique_urls\": int(df[\"url\"].nunique()) if not df.empty else 0,\n",
        "        \"errors\": int(errors),\n",
        "        \"elapsed_seconds\": round(time.perf_counter() - t0, 2),\n",
        "        \"output_csv\": OUTPUT_CSV,\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== SCRAPE REPORT ===\")\n",
        "    for k, v in report.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    if not df.empty:\n",
        "        print(\"\\nSAMPLE ROWS:\")\n",
        "        print(df.head(5).to_string(index=False))\n",
        "\n",
        "    return df, report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_actions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pT6MVZyazPc",
        "outputId": "2f38d414-db71-44a0-9ada-77ec0f06257f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Page 1: found 20 action cards, added 20, total 20\n",
            "[INFO] Page 2: found 20 action cards, added 20, total 40\n",
            "[INFO] Page 3: found 20 action cards, added 20, total 60\n",
            "[INFO] Page 4: found 0 action cards, added 0, total 60\n",
            "[INFO] Page 5: found 20 action cards, added 20, total 80\n",
            "[INFO] Page 6: found 20 action cards, added 20, total 100\n",
            "[INFO] Page 7: found 0 action cards, added 0, total 100\n",
            "[INFO] Page 8: found 20 action cards, added 20, total 120\n",
            "[INFO] Page 9: found 20 action cards, added 20, total 140\n",
            "[INFO] Page 10: found 20 action cards, added 20, total 160\n",
            "[INFO] Page 11: found 0 action cards, added 0, total 160\n",
            "[INFO] Page 12: found 20 action cards, added 20, total 180\n",
            "[INFO] Page 13: found 20 action cards, added 20, total 200\n",
            "[INFO] Page 14: found 0 action cards, added 0, total 200\n",
            "[INFO] Page 15: found 20 action cards, added 20, total 220\n",
            "[INFO] Page 16: found 20 action cards, added 20, total 240\n",
            "[INFO] Page 17: found 20 action cards, added 20, total 260\n",
            "[INFO] Page 18: found 0 action cards, added 0, total 260\n",
            "[INFO] Page 19: found 20 action cards, added 20, total 280\n",
            "[INFO] Page 20: found 20 action cards, added 20, total 300\n",
            "[INFO] Page 21: found 20 action cards, added 20, total 320\n",
            "[INFO] Page 22: found 20 action cards, added 20, total 340\n",
            "[INFO] Page 23: found 20 action cards, added 20, total 360\n",
            "[INFO] Page 24: found 20 action cards, added 20, total 380\n",
            "[INFO] Page 25: found 20 action cards, added 20, total 400\n",
            "[INFO] Page 26: found 20 action cards, added 20, total 420\n",
            "[INFO] Page 27: found 20 action cards, added 20, total 440\n",
            "[INFO] Page 28: found 20 action cards, added 20, total 460\n",
            "[INFO] Page 29: found 20 action cards, added 20, total 480\n",
            "[INFO] Page 30: found 20 action cards, added 20, total 500\n",
            "[INFO] Page 31: found 20 action cards, added 20, total 520\n",
            "[INFO] Page 32: found 20 action cards, added 20, total 540\n",
            "[INFO] Page 33: found 20 action cards, added 20, total 560\n",
            "[INFO] Page 34: found 20 action cards, added 20, total 580\n",
            "[INFO] Page 35: found 20 action cards, added 20, total 600\n",
            "[INFO] Page 36: found 20 action cards, added 20, total 620\n",
            "[INFO] Page 37: found 20 action cards, added 20, total 640\n",
            "[INFO] Page 38: found 20 action cards, added 20, total 660\n",
            "[INFO] Page 39: found 20 action cards, added 20, total 680\n",
            "[INFO] Page 40: found 20 action cards, added 20, total 700\n",
            "[INFO] Page 41: found 20 action cards, added 19, total 719\n",
            "[INFO] Page 42: found 20 action cards, added 19, total 738\n",
            "[INFO] Page 43: found 20 action cards, added 20, total 758\n",
            "[INFO] Page 44: found 20 action cards, added 19, total 777\n",
            "[INFO] Page 45: found 20 action cards, added 20, total 797\n",
            "[INFO] Page 46: found 20 action cards, added 19, total 816\n",
            "[INFO] Page 47: found 20 action cards, added 20, total 836\n",
            "[INFO] Page 48: found 20 action cards, added 20, total 856\n",
            "[INFO] Page 49: found 20 action cards, added 19, total 875\n",
            "[INFO] Page 50: found 20 action cards, added 20, total 895\n",
            "[INFO] Page 51: found 0 action cards, added 0, total 895\n",
            "[INFO] Page 52: found 20 action cards, added 20, total 915\n",
            "[INFO] Page 53: found 20 action cards, added 20, total 935\n",
            "[INFO] Page 54: found 20 action cards, added 20, total 955\n",
            "[INFO] Page 55: found 20 action cards, added 20, total 975\n",
            "[INFO] Page 56: found 20 action cards, added 20, total 995\n",
            "[INFO] Page 57: found 20 action cards, added 5, total 1000\n",
            "\n",
            "=== SCRAPE REPORT ===\n",
            "timestamp: 2025-09-29T21:42:04.100512+00:00\n",
            "pages_visited: 57\n",
            "total_collected: 1000\n",
            "unique_urls: 1000\n",
            "errors: 0\n",
            "elapsed_seconds: 108.03\n",
            "output_csv: github_actions_marketplace_raw.csv\n",
            "\n",
            "SAMPLE ROWS:\n",
            "                product_name                                                                                                description                                                               url  page_number\n",
            "              TruffleHog OSS                                       We read every piece of feedback, and take your input very seriously.             https://github.com/marketplace/actions/trufflehog-oss            1\n",
            "               Metrics embed     An infographics generator with 40+ plugins and 300+ options to display stats about your GitHub account              https://github.com/marketplace/actions/metrics-embed            1\n",
            "yq - portable yaml processor                                        create, read, update, delete, merge, validate and do more with yaml https://github.com/marketplace/actions/yq-portable-yaml-processor            1\n",
            "                Super-Linter Super-linter is a ready-to-run collection of linters and code analyzers, to help validate your source code               https://github.com/marketplace/actions/super-linter            1\n",
            "      Gosec Security Checker                                                                            Runs the gosec security checker     https://github.com/marketplace/actions/gosec-security-checker            1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf24e47-fbc1-4885-beb8-ebf719243e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 1000\n",
            "Unique URLs: 1000\n",
            "Invalid URLs: 0\n",
            "Missing names: 0\n",
            "\n",
            "Top pages by count:\n",
            " page_number\n",
            "1     20\n",
            "40    20\n",
            "2     20\n",
            "32    20\n",
            "33    20\n",
            "34    20\n",
            "35    20\n",
            "36    20\n",
            "37    20\n",
            "38    20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== DATA QUALITY REPORT ===\n",
            "source_csv: github_actions_marketplace_raw.csv\n",
            "clean_csv: github_actions_marketplace_clean.csv\n",
            "total_rows_source: 1000\n",
            "initial_quality: {'missing_product_name': 0, 'missing_description': 0, 'missing_url': 0, 'missing_page_number': 0, 'duplicate_url_rows': 0, 'invalid_url_rows': 0, 'non_numeric_page_rows': 0}\n",
            "dropped_rows_missing_required_fields: 0\n",
            "page_number_na_fixed: 0\n",
            "duplicates_removed_by_url: 0\n",
            "final_row_count: 1000\n",
            "required_columns_present: True\n",
            "notes: Preprocessing: strip HTML (with lxml fallback), lowercase, remove punctuation & extra spaces, tokenize (wordpunct_tokenize), remove stopwords, lemmatize. 'clean_text' is canonical.\n",
            "\n",
            "SAMPLE (product_name, url, clean_text):\n",
            "                product_name                                                               url                                                                                     clean_text\n",
            "              TruffleHog OSS             https://github.com/marketplace/actions/trufflehog-oss                                   trufflehog os read every piece feedback take input seriously\n",
            "               Metrics embed              https://github.com/marketplace/actions/metrics-embed         metric embed infographics generator 40 plugins 300 option display stats github account\n",
            "yq - portable yaml processor https://github.com/marketplace/actions/yq-portable-yaml-processor                       yq portable yaml processor create read update delete merge validate yaml\n",
            "                Super-Linter               https://github.com/marketplace/actions/super-linter super linter super linter ready run collection linters code analyzer help validate source code\n",
            "      Gosec Security Checker     https://github.com/marketplace/actions/gosec-security-checker                                              gosec security checker run gosec security checker\n"
          ]
        }
      ],
      "source": [
        "# PART-2: Preprocess + Data Quality for GitHub Marketplace Actions CSV (model-free tokenizer)\n",
        "#   pip install pandas nltk beautifulsoup4\n",
        "\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, List\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup, FeatureNotFound, MarkupResemblesLocatorWarning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import wordpunct_tokenize  # <-- doesn't require punkt / punkt_tab\n",
        "\n",
        "# ---------- Setup NLTK resources ----------\n",
        "try:\n",
        "    _ = stopwords.words(\"english\")\n",
        "except LookupError:\n",
        "    nltk.download(\"stopwords\")\n",
        "try:\n",
        "    nltk.data.find(\"corpora/wordnet\")\n",
        "except LookupError:\n",
        "    nltk.download(\"wordnet\")\n",
        "\n",
        "RAW_CSV   = \"github_actions_marketplace_raw.csv\"\n",
        "CLEAN_CSV = \"github_actions_marketplace_clean.csv\"\n",
        "REPORT_JSON = \"github_actions_marketplace_quality_report.json\"\n",
        "\n",
        "REQUIRED_COLS = [\"product_name\", \"description\", \"url\", \"page_number\"]\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def strip_html(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text.strip()\n",
        "    # If it doesn’t look like HTML, don’t parse it at all\n",
        "    if \"<\" not in t and \">\" not in t:\n",
        "        return t\n",
        "    try:\n",
        "        return BeautifulSoup(t, \"lxml\").get_text(\" \", strip=True)\n",
        "    except FeatureNotFound:\n",
        "        return BeautifulSoup(t, \"html.parser\").get_text(\" \", strip=True)\n",
        "\n",
        "def normalize_spaces(text: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def normalize_url(u: str) -> str:\n",
        "    if not isinstance(u, str):\n",
        "        return \"\"\n",
        "    u = u.strip()\n",
        "    if not u:\n",
        "        return \"\"\n",
        "    if u.startswith((\"http://\",\"https://\")):\n",
        "        return u\n",
        "    return f\"https://github.com{u if u.startswith('/') else '/' + u}\"\n",
        "\n",
        "def tokenize_lower(text: str) -> List[str]:\n",
        "    \"\"\"Lowercase + remove punctuation to spaces + split with wordpunct tokenizer (no models).\"\"\"\n",
        "    text = (text or \"\").lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # keep letters/digits/spaces\n",
        "    text = normalize_spaces(text)\n",
        "    return wordpunct_tokenize(text) if text else []\n",
        "\n",
        "def remove_stop_and_short(tokens: List[str]) -> List[str]:\n",
        "    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
        "\n",
        "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
        "    return [LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "\n",
        "def preprocess_text(name: str, desc: str) -> Dict[str, str]:\n",
        "    base = f\"{strip_html(name or '')} {strip_html(desc or '')}\".strip()\n",
        "    tokens = tokenize_lower(base)\n",
        "    tokens_ns = remove_stop_and_short(tokens)\n",
        "    lemmas = lemmatize_tokens(tokens_ns)\n",
        "    return {\n",
        "        \"clean_norm\": \" \".join(tokens),\n",
        "        \"tokens_no_stop\": \" \".join(tokens_ns),\n",
        "        \"lemmatized_text\": \" \".join(lemmas),\n",
        "        \"clean_text\": \" \".join(lemmas),\n",
        "    }\n",
        "\n",
        "def quality_checks(df: pd.DataFrame) -> Dict[str, int]:\n",
        "    metrics = {}\n",
        "    for col in REQUIRED_COLS:\n",
        "        metrics[f\"missing_{col}\"] = int(df[col].isna().sum() + (df[col].astype(str).str.len() == 0).sum())\n",
        "    metrics[\"duplicate_url_rows\"] = int(df.duplicated(subset=[\"url\"]).sum())\n",
        "    # ✅ correct: apply ~ to the boolean mask BEFORE summing\n",
        "    invalid_url_mask = ~df[\"url\"].astype(str).str.startswith((\"http://\", \"https://\"))\n",
        "    metrics[\"invalid_url_rows\"] = int(invalid_url_mask.sum())\n",
        "    metrics[\"non_numeric_page_rows\"] = int(pd.to_numeric(df[\"page_number\"], errors=\"coerce\").isna().sum())\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def run_pipeline():\n",
        "    df = pd.read_csv(RAW_CSV)\n",
        "\n",
        "    # Quick raw sanity (no clean_text yet!)\n",
        "    print(\"Rows:\", len(df))\n",
        "    print(\"Unique URLs:\", df[\"url\"].nunique())\n",
        "    print(\"Invalid URLs:\", (~df[\"url\"].astype(str).str.startswith((\"http://\",\"https://\"))).sum())\n",
        "    print(\"Missing names:\", (df[\"product_name\"].astype(str).str.len()==0).sum())\n",
        "    print(\"\\nTop pages by count:\\n\", df[\"page_number\"].value_counts().head(10))\n",
        "\n",
        "    # ----- Quality & preprocessing pipeline -----\n",
        "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns in {RAW_CSV}: {missing}\")\n",
        "\n",
        "    for c in [\"product_name\",\"description\",\"url\"]:\n",
        "        df[c] = df[c].astype(str).str.strip()\n",
        "    df[\"url\"] = df[\"url\"].map(normalize_url)\n",
        "\n",
        "    df[\"page_number\"] = pd.to_numeric(df[\"page_number\"], errors=\"coerce\")\n",
        "\n",
        "    initial_quality = quality_checks(df)\n",
        "\n",
        "    req_mask = (df[\"product_name\"].astype(str).str.len() > 0) & (df[\"url\"].astype(str).str.len() > 0)\n",
        "    dropped_missing_required = int((~req_mask).sum())\n",
        "    df = df.loc[req_mask].copy()\n",
        "\n",
        "    df[\"description\"] = df[\"description\"].fillna(\"\")\n",
        "    na_pages = int(df[\"page_number\"].isna().sum())\n",
        "    df[\"page_number\"] = df[\"page_number\"].fillna(-1).astype(int)\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"url\"], keep=\"first\").reset_index(drop=True)\n",
        "    dups_removed = before - len(df)\n",
        "\n",
        "    # Create cleaned text columns\n",
        "    pre = df.apply(lambda r: preprocess_text(r[\"product_name\"], r[\"description\"]), axis=1, result_type=\"expand\")\n",
        "    df_clean = pd.concat([df, pre], axis=1)\n",
        "\n",
        "    # Save\n",
        "    df_clean.to_csv(CLEAN_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    report = {\n",
        "        \"source_csv\": RAW_CSV,\n",
        "        \"clean_csv\": CLEAN_CSV,\n",
        "        \"total_rows_source\": int(len(pd.read_csv(RAW_CSV))),\n",
        "        \"initial_quality\": initial_quality,\n",
        "        \"dropped_rows_missing_required_fields\": dropped_missing_required,\n",
        "        \"page_number_na_fixed\": na_pages,\n",
        "        \"duplicates_removed_by_url\": int(dups_removed),\n",
        "        \"final_row_count\": int(len(df_clean)),\n",
        "        \"required_columns_present\": all(c in df_clean.columns for c in REQUIRED_COLS),\n",
        "        \"notes\": (\n",
        "            \"Preprocessing: strip HTML (with lxml fallback), lowercase, remove punctuation & extra spaces, \"\n",
        "            \"tokenize (wordpunct_tokenize), remove stopwords, lemmatize. 'clean_text' is canonical.\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    with open(REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"\\n=== DATA QUALITY REPORT ===\")\n",
        "    for k, v in report.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # ✅ Now 'clean_text' exists\n",
        "    print(\"\\nSAMPLE (product_name, url, clean_text):\")\n",
        "    print(df_clean[[\"product_name\", \"url\", \"clean_text\"]].head(5).to_string(index=False))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "# Token + bigram frequency from the clean CSV\n",
        "import re, itertools, collections, pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"github_actions_marketplace_clean.csv\")\n",
        "tokens = (df[\"clean_text\"].fillna(\"\").str.split().tolist())\n",
        "flat = list(itertools.chain.from_iterable(tokens))\n",
        "\n",
        "# top unigrams\n",
        "uni = collections.Counter([t for t in flat if len(t) > 2]).most_common(25)\n",
        "\n",
        "# top bigrams\n",
        "bigrams = []\n",
        "for toks in tokens:\n",
        "    bigrams.extend(zip(toks, toks[1:]))\n",
        "bi = collections.Counter([\" \".join(bg) for bg in bigrams if all(len(w)>2 for w in bg)]).most_common(25)\n",
        "\n",
        "print(\"Top 25 tokens:\")\n",
        "for w,c in uni: print(f\"{w:20s} {c}\")\n",
        "print(\"\\nTop 25 bigrams:\")\n",
        "for w,c in bi: print(f\"{w:25s} {c}\")\n",
        "\n",
        "###################\n",
        "# Adds 'stemmed_text' to your clean CSV\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "df = pd.read_csv(\"github_actions_marketplace_clean.csv\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_line(s: str) -> str:\n",
        "    toks = str(s or \"\").split()\n",
        "    return \" \".join(stemmer.stem(t) for t in toks)\n",
        "\n",
        "df[\"stemmed_text\"] = df[\"clean_text\"].fillna(\"\").map(stem_line)\n",
        "df.to_csv(\"github_actions_marketplace_clean.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Added 'stemmed_text' to github_actions_marketplace_clean.csv\")\n",
        "print(df[[\"product_name\",\"clean_text\",\"stemmed_text\"]].head(3).to_string(index=False))\n",
        "\n",
        "\n",
        "###################\n",
        "# Simple asserts to catch regressions when you re-run scraping\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"github_actions_marketplace_clean.csv\")\n",
        "\n",
        "assert set([\"product_name\",\"description\",\"url\",\"page_number\",\"clean_text\"]).issubset(df.columns), \"Missing expected columns\"\n",
        "assert len(df) >= 500, \"Too few rows collected\"\n",
        "assert df[\"url\"].nunique() == len(df), \"Duplicate URLs present\"\n",
        "assert (~df[\"url\"].astype(str).str.startswith((\"http://\",\"https://\"))).sum() == 0, \"Invalid URLs found\"\n",
        "assert df[\"product_name\"].astype(str).str.len().gt(0).all(), \"Empty product names found\"\n",
        "\n",
        "print(\"All assertions passed \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-NhfURBdb_R",
        "outputId": "8622f25a-9a4f-4805-d9d4-8640af650451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 tokens:\n",
            "action               550\n",
            "github               427\n",
            "run                  144\n",
            "request              111\n",
            "pull                 105\n",
            "build                100\n",
            "code                 99\n",
            "file                 98\n",
            "setup                98\n",
            "deploy               86\n",
            "release              83\n",
            "workflow             72\n",
            "using                65\n",
            "check                62\n",
            "install              57\n",
            "docker               56\n",
            "every                55\n",
            "version              55\n",
            "issue                55\n",
            "read                 53\n",
            "repository           53\n",
            "input                52\n",
            "feedback             49\n",
            "take                 49\n",
            "piece                48\n",
            "\n",
            "Top 25 bigrams:\n",
            "github action             262\n",
            "pull request              103\n",
            "read every                48\n",
            "every piece               48\n",
            "piece feedback            48\n",
            "feedback take             48\n",
            "take input                48\n",
            "input seriously           48\n",
            "action github             47\n",
            "action run                32\n",
            "github page               23\n",
            "docker image              18\n",
            "github release            17\n",
            "action workflow           17\n",
            "code review               17\n",
            "action action             15\n",
            "code coverage             14\n",
            "action build              14\n",
            "specific version          13\n",
            "action read               13\n",
            "using github              12\n",
            "add path                  12\n",
            "action deploy             11\n",
            "coverage report           11\n",
            "workflow run              10\n",
            "Added 'stemmed_text' to github_actions_marketplace_clean.csv\n",
            "                product_name                                                                             clean_text                                                                stemmed_text\n",
            "              TruffleHog OSS                           trufflehog os read every piece feedback take input seriously                   trufflehog os read everi piec feedback take input serious\n",
            "               Metrics embed metric embed infographics generator 40 plugins 300 option display stats github account metric emb infograph gener 40 plugin 300 option display stat github account\n",
            "yq - portable yaml processor               yq portable yaml processor create read update delete merge validate yaml            yq portabl yaml processor creat read updat delet merg valid yaml\n",
            "All assertions passed \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab888b9-6b3f-49dd-8c58-867eea5d3fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweepy\n",
            "  Downloading tweepy-4.16.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n",
            "Downloading tweepy-4.16.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tweepy\n",
            "Successfully installed tweepy-4.16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3/dist-packages/blinker/base.py:96: SyntaxWarning: invalid escape sequence '\\*'\n",
            "  `sender=` as a single positional argument and any \\*\\*kwargs that\n",
            "/usr/lib/python3/dist-packages/blinker/base.py:174: SyntaxWarning: invalid escape sequence '\\*'\n",
            "  `sender=` as a single positional argument and any \\*\\*kwargs that\n",
            "/usr/lib/python3/dist-packages/blinker/base.py:242: SyntaxWarning: invalid escape sequence '\\*'\n",
            "  \"\"\"Emit this signal on behalf of *sender*, passing on \\*\\*kwargs.\n",
            "/tmp/ipython-input-3585975501.py:21: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
            "  ts = dt.datetime.utcfromtimestamp(reset).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Preflight] 429. Waiting 899s until 2025-09-29 22:09:30 UTC …\n",
            "Hit rate limit during pagination. Stopping.\n",
            "Saved 0 tweets → tweets_raw.csv\n"
          ]
        }
      ],
      "source": [
        "!pip install tweepy\n",
        "\n",
        "import os, time, requests, datetime as dt, tweepy, pandas as pd\n",
        "\n",
        "\n",
        "QUERY = \"(#machinelearning OR #MachineLearning OR #AI OR #ArtificialIntelligence) -is:retweet lang:en\"\n",
        "MAX_TWEETS = 200       # keep low on tight quotas\n",
        "REQUESTS_DELAY = 1.2\n",
        "OUT_CSV = \"tweets_raw.csv\"\n",
        "\n",
        "def preflight_or_wait(token):\n",
        "    r = requests.get(\n",
        "        \"https://api.x.com/2/tweets/search/recent\",\n",
        "        params={\"query\": \"AI -is:retweet\", \"max_results\": 10},\n",
        "        headers={\"Authorization\": f\"Bearer \" + token},\n",
        "        timeout=20,\n",
        "    )\n",
        "    if r.status_code == 429:\n",
        "        reset = int(r.headers.get(\"x-rate-limit-reset\", \"0\"))\n",
        "        wait = max(reset - int(time.time()), 0)\n",
        "        ts = dt.datetime.utcfromtimestamp(reset).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "        print(f\"[Preflight] 429. Waiting {wait}s until {ts} …\")\n",
        "        time.sleep(wait + 2)\n",
        "    elif r.status_code != 200:\n",
        "        print(\"Probe failed:\", r.status_code, r.text[:400])\n",
        "        raise SystemExit(\"Fix token/plan before continuing.\")\n",
        "\n",
        "preflight_or_wait(BEARER_TOKEN)\n",
        "\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=False)\n",
        "\n",
        "tweets, next_token, total = [], None, 0\n",
        "while total < MAX_TWEETS:\n",
        "    try:\n",
        "        resp = client.search_recent_tweets(\n",
        "            query=QUERY,\n",
        "            expansions=[\"author_id\"],\n",
        "            tweet_fields=[\"id\",\"text\",\"created_at\",\"lang\",\"author_id\"],\n",
        "            user_fields=[\"username\",\"name\"],\n",
        "            max_results=100,\n",
        "            next_token=next_token\n",
        "        )\n",
        "    except tweepy.TooManyRequests as e:\n",
        "        # Stop instead of long sleeps inside notebooks\n",
        "        print(\"Hit rate limit during pagination. Stopping.\")\n",
        "        break\n",
        "\n",
        "    if not resp.data:\n",
        "        break\n",
        "\n",
        "    users = {u.id: u.username for u in (resp.includes.get(\"users\", []) if resp.includes else [])}\n",
        "    for t in resp.data:\n",
        "        tweets.append({\"tweet_id\": t.id, \"username\": users.get(t.author_id, \"\"), \"text\": t.text})\n",
        "        total += 1\n",
        "        if total >= MAX_TWEETS:\n",
        "            break\n",
        "\n",
        "    next_token = resp.meta.get(\"next_token\")\n",
        "    if not next_token:\n",
        "        break\n",
        "    time.sleep(REQUESTS_DELAY)\n",
        "\n",
        "pd.DataFrame(tweets, columns=[\"tweet_id\",\"username\",\"text\"]).to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved {len(tweets)} tweets → {OUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Clean tweet text, run data-quality checks, save clean CSV\n",
        "# pip install pandas nltk beautifulsoup4\n",
        "\n",
        "import re, json, warnings\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "# one-time downloads (safe to re-run)\n",
        "try: _ = stopwords.words(\"english\")\n",
        "except LookupError: nltk.download(\"stopwords\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
        "\n",
        "IN_CSV   = \"tweets_raw.csv\"\n",
        "OUT_CSV  = \"tweets_clean.csv\"\n",
        "REPORT   = \"tweets_quality_report.json\"\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "\n",
        "def strip_html(text: str) -> str:\n",
        "    t = str(text or \"\").strip()\n",
        "    if \"<\" not in t and \">\" not in t:  # skip BS if not HTML-like\n",
        "        return t\n",
        "    return BeautifulSoup(t, \"html.parser\").get_text(\" \", strip=True)\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = strip_html(s)\n",
        "    s = s.lower()\n",
        "    # remove URLs\n",
        "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)\n",
        "    # remove mentions and hashtag symbols (keep the term after #)\n",
        "    s = re.sub(r\"@\\w+\", \" \", s)\n",
        "    s = re.sub(r\"#\", \" \", s)\n",
        "    # remove non-alphanumeric except spaces\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "    # collapse whitespace\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenize_no_stop(s: str):\n",
        "    toks = wordpunct_tokenize(s) if s else []\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 1]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "# ----- LOAD -----\n",
        "df = pd.read_csv(IN_CSV)\n",
        "\n",
        "# ----- BASIC QUALITY & CONSISTENCY -----\n",
        "required = [\"tweet_id\",\"username\",\"text\"]\n",
        "missing_cols = [c for c in required if c not in df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing columns: {missing_cols}\")\n",
        "\n",
        "# drop rows with empty critical fields\n",
        "df[\"tweet_id\"] = pd.to_numeric(df[\"tweet_id\"], errors=\"coerce\")\n",
        "df[\"username\"] = df[\"username\"].astype(str).str.strip()\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "before = len(df)\n",
        "df = df.dropna(subset=[\"tweet_id\",\"username\",\"text\"])\n",
        "df = df[df[\"username\"] != \"\"]\n",
        "dropped_missing = before - len(df)\n",
        "\n",
        "# remove duplicates by tweet_id\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(subset=[\"tweet_id\"]).reset_index(drop=True)\n",
        "dups_removed = before - len(df)\n",
        "\n",
        "# ----- CLEANING -----\n",
        "df[\"text_clean\"] = df[\"text\"].map(clean_text)\n",
        "df[\"text_tokens\"] = df[\"text_clean\"].map(tokenize_no_stop)\n",
        "\n",
        "# Final sanity: no empty text after cleaning\n",
        "before = len(df)\n",
        "df = df[df[\"text_clean\"].str.len() > 0].reset_index(drop=True)\n",
        "empties_removed = before - len(df)\n",
        "\n",
        "# ----- SAVE CLEAN -----\n",
        "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# ----- REPORT -----\n",
        "report = {\n",
        "    \"rows_input\": int(pd.read_csv(IN_CSV).shape[0]),\n",
        "    \"rows_after_drop_missing\": int(len(df) + dropped_missing + dups_removed + empties_removed),\n",
        "    \"dropped_missing_required_fields\": int(dropped_missing),\n",
        "    \"duplicates_removed_by_tweet_id\": int(dups_removed),\n",
        "    \"rows_removed_empty_after_clean\": int(empties_removed),\n",
        "    \"final_rows\": int(len(df)),\n",
        "    \"columns\": list(df.columns),\n",
        "    \"notes\": \"Cleaned with HTML strip, lowercase, URL/mention/hashtag removal, punctuation trim, whitespace collapse; tokenized and stopwords removed.\"\n",
        "}\n",
        "with open(REPORT, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f\"Saved clean data → {OUT_CSV}\")\n",
        "print(\"Sample:\")\n",
        "print(df.head(5).to_string(index=False))\n",
        "print(\"\\nReport:\", json.dumps(report, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO295wlAI4VF",
        "outputId": "493899a0-6f83-4a4d-f4ec-5f48757fd192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved clean data → tweets_clean.csv\n",
            "Sample:\n",
            "Empty DataFrame\n",
            "Columns: [tweet_id, username, text, text_clean, text_tokens]\n",
            "Index: []\n",
            "\n",
            "Report: {\n",
            "  \"rows_input\": 0,\n",
            "  \"rows_after_drop_missing\": 0,\n",
            "  \"dropped_missing_required_fields\": 0,\n",
            "  \"duplicates_removed_by_tweet_id\": 0,\n",
            "  \"rows_removed_empty_after_clean\": 0,\n",
            "  \"final_rows\": 0,\n",
            "  \"columns\": [\n",
            "    \"tweet_id\",\n",
            "    \"username\",\n",
            "    \"text\",\n",
            "    \"text_clean\",\n",
            "    \"text_tokens\"\n",
            "  ],\n",
            "  \"notes\": \"Cleaned with HTML strip, lowercase, URL/mention/hashtag removal, punctuation trim, whitespace collapse; tokenized and stopwords removed.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f76ec54c32245fd92c8717dd8295053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a35880903fa045369aceb99f3beb0b3b",
              "IPY_MODEL_f611d822b52742a5b345a39e5c4d8dfe",
              "IPY_MODEL_2446027c5d06448a81a02c75ba0895eb"
            ],
            "layout": "IPY_MODEL_c9c12565bed1484db0e2f0783732ce2d"
          }
        },
        "a35880903fa045369aceb99f3beb0b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f652612edc4d1d832e2412061bbe02",
            "placeholder": "​",
            "style": "IPY_MODEL_a31b5603fd424ac7b6f82756818bd1c6",
            "value": "Collecting narrators: 100%"
          }
        },
        "f611d822b52742a5b345a39e5c4d8dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55ebe9fb07254c0aa31ee43380966e26",
            "max": 1009,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e5a3561dee64080acf8658c16e17bcc",
            "value": 1009
          }
        },
        "2446027c5d06448a81a02c75ba0895eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_964bdf5d08e54307bdcd9499ec57a755",
            "placeholder": "​",
            "style": "IPY_MODEL_814292dfbf92466abddf8fd45f47958b",
            "value": " 1009/1009 [00:42&lt;00:00, 19.18it/s]"
          }
        },
        "c9c12565bed1484db0e2f0783732ce2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f652612edc4d1d832e2412061bbe02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31b5603fd424ac7b6f82756818bd1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ebe9fb07254c0aa31ee43380966e26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5a3561dee64080acf8658c16e17bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "964bdf5d08e54307bdcd9499ec57a755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814292dfbf92466abddf8fd45f47958b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a167a47fc4f4e2d83e7f441df72a94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33143a58e40942898a4f69f7870132e2",
              "IPY_MODEL_4028a36450be484a9b3571edc20a631a",
              "IPY_MODEL_cdc38f5f74fc4c23a680e6ad38c4b263"
            ],
            "layout": "IPY_MODEL_e796cdad3b004112828aea4840481e7a"
          }
        },
        "33143a58e40942898a4f69f7870132e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914c696deb2d419db24db030eb935415",
            "placeholder": "​",
            "style": "IPY_MODEL_839ab050c3fa46e0b17c2eb95772857d",
            "value": "100%"
          }
        },
        "4028a36450be484a9b3571edc20a631a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_317c2f8336074823895fbd1d48693750",
            "max": 1005,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcd2bec1ccda4d258517a7e50c2e77ae",
            "value": 1005
          }
        },
        "cdc38f5f74fc4c23a680e6ad38c4b263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c14733e7e542b89ab894d6b8d3ceee",
            "placeholder": "​",
            "style": "IPY_MODEL_fd25753fc96a4bbb89e4fb5138ef44a2",
            "value": " 1005/1005 [23:11&lt;00:00,  1.41s/it]"
          }
        },
        "e796cdad3b004112828aea4840481e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914c696deb2d419db24db030eb935415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839ab050c3fa46e0b17c2eb95772857d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "317c2f8336074823895fbd1d48693750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd2bec1ccda4d258517a7e50c2e77ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61c14733e7e542b89ab894d6b8d3ceee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd25753fc96a4bbb89e4fb5138ef44a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
